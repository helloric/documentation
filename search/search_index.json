{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HelloRIC - Documentation","text":"<p>HelloRic is a robotic student project hosted by the University of Bremen in collaboration with the DFKI Robotics Innovation Center in Bremen. The goal of the project is to create a robotic tour guid for the DFKI Robotics Innovation Center which can show guests around.</p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>This documentation is entrypoint for future projects that want to use or extend the existing hard- and software, user of the system and other projects or individuals that want to develop similar robotic systems.</p> <p>THIS IS CURRENTLY WORK IN PROGRESS, HERE YOU WILL FIND AN ARCHITECTURE AND HARDWARE OVERVIEW AS WELL AS AN OUTLINE POINTING TO THE SECTIONS OF THE DOCUMENTATION IN THE FUTURE.</p>"},{"location":"emotion-ui/animation/","title":"Animation","text":"<p>This section covers how the animations, namely the blinking and the speaking of RICBOT were created. The \"animations\" itself are pretty basic. Just some SVGs which are switched in and out depending on some variables. But it helps to make RICBOT more humanlike (and it looks fancy too). Every function necessary is found in the <code>faceanimation.js</code> inside the lib folder. They are used in the <code>botface.svelte</code>, located in the components folder inside lib .</p>"},{"location":"emotion-ui/animation/#blinking","title":"Blinking","text":"<p>In a nutshell, the blinking works by setting up a Svelte Store with a boolean. The value of the boolean is set depending on an interval. If it's <code>false</code> the normal eyes are displayed and if it's <code>true</code> the <code>eye_sleeping.svg</code> is switched in for the normal eyes. Except 'amused' and 'excited', every emotion is able to blink.</p> <p>The heart of the blinking animation is the <code>doBlinking()</code> function:</p> <pre><code>export function doBlinking(interval = 5000, duration = 200) {\n\n\u00a0 const isBlinking = writable(false);\n\n\u00a0\n\n\u00a0 function setupBlinking() {\n\n\u00a0\u00a0\u00a0 const blink = () =&gt; {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 isBlinking.set(true);\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 setTimeout(() =&gt; {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 isBlinking.set(false);\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 }, duration);\n\n\u00a0\u00a0\u00a0 };\n\n\u00a0\n\n\u00a0\u00a0\u00a0 const blinkInterval = setInterval(blink, interval);\n\n\u00a0\n\n\u00a0\n\n\u00a0\u00a0\u00a0 return () =&gt; {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 clearInterval(blinkInterval);\n\n\u00a0\u00a0\u00a0 };\n\n\u00a0 }\n\n\u00a0 return { isBlinking, setupBlinking };\n\n}\n\n\u00a0\n</code></pre> <p>Here are some key facts for better understanding the function:</p> <ul> <li> <p><code>interval</code> is the passing time between two blinks</p> </li> <li> <p><code>duration</code> is, well, the duration of the blink.</p> </li> <li> <p><code>isBlinking</code> is the Svelte Store/boolean</p> </li> <li> <p><code>setupBlinking()</code> and <code>blink</code> are setting the store <code>true</code> and starting a timer. After the timer is finished, the store is set to <code>false</code> again.</p> </li> <li> <p><code>blinkInterval</code> sets an interval to call <code>blink</code> every length of <code>interval</code></p> </li> <li> <p><code>clearInterval(blinkInterval)</code> resets the interval when it's finished, so a new one can be created</p> </li> <li> <p>At the end, <code>isBlinking</code> and <code>setupBlinking</code> are returned so they can be used by other scripts</p> </li> </ul>"},{"location":"emotion-ui/animation/#speaking","title":"Speaking","text":"<p>The speaking animation works at its core pretty much the same as the blinking animation. The only difference is that the speaking can be turned on or off. Would be pretty silly if RICBOT would do the speak animation without saying anything, don't you think?</p> <p>Let\u2019s look at the function in detail:</p> <pre><code>let speakInterval;\n\nlet duration = 200;\n\nlet interval = 450;\n\n\u00a0\n\nexport function doSpeaking() {\n\n\u00a0\n\n\u00a0 changeState();\n\n\u00a0\n\n\u00a0 if (conversation) {\n\n\u00a0\u00a0\u00a0 const speach = () =&gt; {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 isSpeaking.set(true);\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 setTimeout(() =&gt; {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 isSpeaking.set(false);\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 }, duration);\n\n\u00a0\u00a0\u00a0 };\n\n\u00a0\n\n\u00a0\u00a0\u00a0 if (!speakInterval) {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 speakInterval = setInterval(speach, interval);\n\n\u00a0\u00a0\u00a0 }\n\n\u00a0\n\n\u00a0 }\n\n\u00a0 else {\n\n\u00a0\u00a0\u00a0 isSpeaking.set(false);\n\n\u00a0\n\n\u00a0\u00a0\u00a0 if (speakInterval) {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 clearInterval(speakInterval);\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 speakInterval = undefined;\n\n\u00a0\u00a0\u00a0 }\n\n\u00a0 }\n\n}\n</code></pre> <p>Here are the facts that differ from <code>doBlinking()</code>:</p> <ul> <li> <p><code>speakInterval</code> is declared outside to prevent multiple ones existing at the same time</p> </li> <li> <p><code>interval</code> and <code>duration</code> are declared outside as well to make the animation run smoother (somehow)</p> </li> <li> <p><code>changeState()</code> alters the boolean <code>conversation</code> which checks if RICBOT is currently speaking or not. If it's true, the same stuff as the blinking happens</p> </li> </ul> <p>If a <code>speakInterval</code> does not already exist, a new one is created</p> <p>If <code>conversation</code> is false, the Svelte Store for speaking is set to <code>false</code> and any existing speakIntervals are terminated.</p>"},{"location":"emotion-ui/animation/#inside-botfacesvelte","title":"Inside <code>Botface.svelte</code>","text":"<p>The <code>doBlinking()</code> function is imported fully for the blinking. For the speaking, it is sufficient to import <code>isSpeaking</code> meaning only the status of the Svelte Store (<code>true</code> or <code>false</code>).</p> <p>Next, a constant is declared which allows us to access <code>isBlinking</code> and <code>setupBlinking()</code>.</p> <p>In the <code>onMount()</code> function, <code>setupBlinking()</code> is called. This starts the blinking animation when the website is loaded. The blinking can't be terminated and runs constantly.</p> <p>Now to where the magic happens:</p> <pre><code>$: {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 robotFace(emotion);\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if ($isBlinking &amp;&amp; blinking) {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 eye_class = 'stroke';\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 eyeR = eyeL = eye_blinking;\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\n\u00a0\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if ($isSpeaking) {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mouth_class = 'strokefill'\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mouth = mouth_speaking;\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\n\u00a0\u00a0\u00a0 }\n</code></pre> <p>This function is always called when something changes on the website. First, the current robotFace according to the current emotion, is displayed. If <code>isBlinking</code> is currently true and the emotion has the ability to blink, the current eyes are swapped for the blinking eyes. The <code>eye_class</code> is changed too so that the blinking eyes can be colored properly.</p> <p>The same happens with the mouth while <code>isSpeaking</code> is true.</p>"},{"location":"emotion-ui/assets/","title":"Assets","text":""},{"location":"emotion-ui/assets/#how-to-find","title":"How to find:","text":"<p>Navigate to src , then lib . Then you'll find the assets folder.</p>"},{"location":"emotion-ui/assets/#whats-inside","title":"What's inside?","text":"<p>Depending on the emotion RICBOT uses different eyes or a different mouth. All the files for those can be found inside this folder. All of them are SVG graphics. </p> <p>The files inside are:  - <code>base.svg</code> for RICBOTs facial base - <code>eye_amused.svg</code> and <code>mouth_amused.svg</code> for the emotion amused - <code>eye_bored.svg</code> and <code>mouth_bored.svg</code> for the emotion bored - <code>eye_calm.svg</code> and <code>mouth_calm.svg</code> for the emotion calm - <code>eye_excited.svg</code> and <code>mouth_excited.svg</code> for the emotion excited - <code>eye_frustrated.svg</code> and <code>mouth_frustrated.svg</code> for the emotion frustrated - <code>eye_happy.svg</code> and <code>mouth_happy.svg</code> for the emotion happy - <code>eye_sad.svg</code> and <code>mouth_sad.svg</code> for the emotion sad - <code>eye_worried.svg</code> and <code>mouth_worried.svg</code> for the emotion worried - <code>eye_sleeping.svg</code> and <code>mouth_speeping.svg</code> which are used as the assets for blinking and speaking - <code>eye.thinking_r.svg</code> and <code>mouth_thinking.svg</code> which are used to create the thinking emotion -  <code>eye_confused.svg</code> and <code>mouth_confused.svg</code> which are used to create the confused emotion</p> <p>\"Thinking\" extensions for a smoother transition from one emotion to the <code>thinking</code>emotion:</p> <ul> <li><code>thinking_happy</code>, <code>thinking_amused</code>, <code>thinking_worried</code>, <code>thinking_excited</code>,<code>thinking_frustrated</code>,<code>thinking_confused</code>,<code>thinking_sad</code>,<code>thinking_calm</code>and <code>thinking_bored</code> </li> </ul>"},{"location":"emotion-ui/assets/#nice-to-keep-in-mind","title":"Nice to keep in mind:","text":"<p>If you take a look inside the individual SVG files, you'll notice the different object properties and color properties for each of them.</p> <p>For example:<code>ellipse cx=\"21.809\" cy=\"31.511\" rx=\"21.809\" ry=\"31.511\"</code> and <code>fill=\"none\" stroke=\"#70c1e2\"</code></p> <p>Depending on those parameters, different CSS functions are used later to color the pieces. Let's look at the color properties: Either fill is used, stroke is used, or both of them are used. For each of those cases, there are different CCS functions used to color the SVG. This allows RICBOT to have more than the default light blue face color. This will be expanded on in \"The CSS behind\". Just keep the importance of the color properties in mind. </p>"},{"location":"emotion-ui/audio_manager/","title":"The Audio Manager","text":""},{"location":"emotion-ui/audio_manager/#motivation","title":"Motivation","text":"<p>For the robot to actually hear us, we need some functionality to record our audio data. However, we shouldn't record all the audio data, but only the spoken one. Otherwise, we'd constantly hear the robot say something even though no one asked.</p>"},{"location":"emotion-ui/audio_manager/#libraries","title":"Libraries","text":"<p>Audio Processing is done via. the WebAudio API.</p> <p>On the main page script, you'll see that we request the microphone itself twice. This is because, at this moment, the Browser API does not provide a way to request the permissions for microphone access alone.</p> <p>Warning</p> <p>The WebAudio API can only be called from a secure context. Those include <code>https://</code> domains and the <code>localhost</code>. For this reason, you need to get an SSL-Certificate first, if you plan to make this website publicly accessible outside the robot's system</p>"},{"location":"emotion-ui/audio_manager/#the-audio-manager","title":"The Audio Manager","text":"<p>The Audio Manager loads a new Audio Context. An Audio Context provides a graphical representation of how audio flows. We can insert and connect new nodes as we please. Generally, the representation looks like this, where the circles represent the nodes themselves, rounded rectangles represent the callable input endpoints of the worklet, and the rhombusses represent the output endpoints of the worklet.</p> <pre><code>graph LR\n    SourceNode((MediaStreamAudioSourceNode)) --&gt;|\"Audio-Input\"| WorkletNode((\"AudioWorkletNode (Speech Controller)\"));\n    WorkletNode --&gt;|\"Decibel-Measurement\"| Outgoing1{\"update_decibels\"};\n    WorkletNode --&gt;|\"Speech\"| Outgoing2{\"audio_available\"};\n    Incoming1(\"update_threshold\") --&gt;|\"Decibel Threshold\"| WorkletNode;\n    Incoming2(\"update_sample_rate\") --&gt;|\"Sample Rate\"| WorkletNode;\n    Incoming3(\"unblock_microphone\") --&gt; WorkletNode;\n    Incoming4(\"block_microphone\") --&gt; WorkletNode;</code></pre>"},{"location":"emotion-ui/audio_manager/#the-worklet-speech-controller","title":"The Worklet (Speech Controller)","text":"<p>The Worklet processes chunks of audio-data. The WebAudio-API uses a default chunk-size of 128 frames. </p> <p>We provided a configurable option, such that we have some decibel threshold (you can configure it on the <code>/settings</code>-Page). This important line determines whether or not we are speaking. If the audio is above the line, we consider it speech. If the audio is below the line, we consider it non-speech.</p> <p>The worklet handles the microphone in three states:</p> <ul> <li>IDLE: Doesn't collect any data until the decibel threshold is crossed.</li> <li>LISTENING: Collects all data until enough idle seconds (data below the decibel threshold) have passed.</li> <li>BLOCKED: Doesn't collect any data.</li> </ul> <p>While we are IDLE or LISTENING, we listen and measure to a chunk of audio data. This chunk of audio data has some volume that we calculate as decibels. Decibels are a unit of measurement that determines how loud an audio is.</p> <p>We distinguish between pauses and stopped speech by using something that we call a \"pause threshold\". Some amount of seconds that has to pass until we consider non-speaking parts not as a pause, but as stopped speaking altogether. We do so by measuring the point in time when the speaker has spoken. If enough time has passed without speaking (we measure for each incoming frame how much time has elapsed), we send off the audio.</p>"},{"location":"emotion-ui/botface/","title":"The face itself","text":"<p>Everything the face needs is found in the <code>botface.svelte</code> inside the components folder. It's a Svelte component which is displayed additionally to the contents of the <code>+page.svelte</code> main page.</p>"},{"location":"emotion-ui/botface/#the-script-tag","title":"The <code>script</code> tag:","text":"<ol> <li>The necessary scripts, functions and assets are imported.</li> <li>The needed variables are created</li> <li><code>eyeL</code> and <code>eyeR</code> for the assets of the eyes</li> <li><code>mouth</code> for the asset of the mouth</li> <li><code>blinking</code> for the blinking ability</li> <li><code>color</code> is for the default color the emotion should have</li> <li><code>mouth_class</code> and <code>eye_class</code> for the right coloring later in the CSS</li> <li>The animations are handled. See The animation for additional details</li> <li>The <code>robotFace(emotion)</code> function</li> </ol>"},{"location":"emotion-ui/botface/#robotfaceemotion","title":"<code>robotFace(emotion)</code>","text":"<p>The function contains a constant which holds all the available emotions and their needed assets, coloring classes and their blinking ability. Depending on the given <code>emotion</code> the needed parameters are set for display and for the animation.</p>"},{"location":"emotion-ui/botface/#the-html","title":"The HTML","text":"<p>The HTML is a single <code>div</code> that contains the base of the face, the eyes and the mouth. They are displayed as HTML elements and not as simple SVGs. This allows us to recolor them freely, like an HTML element, without changing the color of the SVG.</p>"},{"location":"emotion-ui/docker/","title":"Docker","text":"<p>Docker is a software for containerizing applications. Basically, you put your application in a box and install all the needed dependencies in said box, and Ta Da: You have a completely independent developing environment. This eliminates the famous \"bUt It wOrKs oN mY mAsHiNe\" and makes deploying software programs easier. Just put your built Docker image on the RICBOT and run it. No further installation is needed. Like the whole project, the emotionsystem is also dockerised.</p>"},{"location":"emotion-ui/docker/#the-dockerfile","title":"The Dockerfile","text":"<p>You can find a docker folder inside the repository. Inside, you can find the Dockerfile for the emotionsystem and for the documentation. This is the content of the Dockerfile for the system:</p> <pre><code>FROM node:20.14.0\n\nARG PORT\n\nENV SERVER_PORT=${PORT:-5173}\n\nWORKDIR /root/build\n\nEXPOSE ${SERVER_PORT}\nEXPOSE 7000\n\u00a0\nCOPY . .\n\nRUN chmod +x ./startup.sh\n\nENTRYPOINT [ \"./startup.sh\"]\n</code></pre> <p>First, the needed Node version is installed. Then the port to display the website is set. Then the basic structure of the container is implemented, and the needed ports are set. Everything is then copied into the Docker image, and the <code>startup.sh</code> is called. Inside the <code>startup.sh</code> are the necessary console commands for starting the SvelteKit application.  It looks like this: <pre><code>#! /bin/bash\n\ncd CalculateEmotion\nnpm install\nnpm run build\nnpm run dev -- --host --port ${SERVER_PORT}\n</code></pre> This starts the application automatically and hosts it on your network when opening the container. With this already done, RICBOT only has to access the site the application is hosted on. And voil\u00e0, there is the face!</p>"},{"location":"emotion-ui/docker/#how-to-access","title":"How to access","text":"<p>After cloning the emotion repository, go to the official Docker website and download Docker Desktop. After that, start Docker Desktop, open the repository in Visual Studio Code and type the following two things in the console: <pre><code>docker compose build\n</code></pre> This can take a while, depending on your mashine. When the building process is finished type <pre><code>docker compose up\n</code></pre> to start the containerized application. </p>"},{"location":"emotion-ui/docker/#necessary-to-keep-in-mind","title":"Necessary to Keep in Mind","text":"<p>Inside the Dockerfile, the web application has already been built. It now won't change itself if you alter the code because it is in \"deployment mode\" so to speak. To have a better experience during development, don't work inside the docker container and use <code>npm run dev</code> to access the website. When you're finished, or when you want to test stuff inside the container, just rebuild the Docker image with <code>docker compose build</code> again and use your new Docker image. </p>"},{"location":"emotion-ui/getting-Started/","title":"What is my purpose?","text":"<p>This web application was created so that RICBOT can have emotions and display them to the visitors during the tour. This aims to make RICBOT more human-like and thus boost his \"acceptance\" among humans. </p> <p>RICBOT has nine different emotions: - amused - bored - calm - excited - frustrated - happy - sad - worried - thinking</p> <p>The first eight emotions are based on the previous iteration of the HelloRIC project and were adapted for our iteration. The table created then can be seen in the image below. We added the thinking emotion for user feedback purposes. Whenever the LLM starts to generate the output for RICBOT we display the thinking emotion. That way, the user gets feedback that something is happening and does not think the robot froze. </p> <p></p> <p>The current emotion can be selected in two ways. Either by selecting it directly or by calculating it based on the two values <code>arousal</code> and <code>valence</code>. <code>arousal</code> and <code>valence</code> both have a range from -4 to 3. 0 being on the top of the x-aches. The emotion is then selected depending on the value combination seen above.</p> <p>Depending on the social context, the LLM behind RICBOT's speech adjusts the emotion RICBOT is currently feeling to influence the output of the LLM. The LLM then sends a ROS Message to the emotion system to update the displayed emotion. In this version, no calculation is necessary to display an emotion. We had to cut this feature due to manpower issues, but we left everything we already did in the project itself. Feel free to reuse it and expand the UI with it :)</p>"},{"location":"emotion-ui/router/","title":"How the emotions are selected","text":"<p>The selection of the current emotion is done in the <code>router.js</code> file inside the lib folder. The current emotion can either be chosen directly or can be determined by the current levels of <code>arousal</code> and <code>valence</code> (See Purpose for additional information)</p>"},{"location":"emotion-ui/router/#the-actual-used-code-at-the-moment","title":"The actual used code at the moment","text":"<p><pre><code>export let emotionsNumber = 2;\n\nexport function setEmotion(index) {\n\u00a0 emotionsNumber = index;\n}\n</code></pre> This sets the initial emotion. The registered values for <code>emotionsNumber</code> are 0 to 8. 8 being thinking. The other emotions are sorted alphabetically: </p> <ol> <li>Amused</li> <li>Bored</li> <li>Calm</li> <li>Excited</li> <li>Frustrated</li> <li>Happy</li> <li>Sad</li> <li>Worried</li> <li>Thinking \u00a0 \u00a0  Thinking is at the end because it is just a filler emotion for when RICBOTs LLM is generating and is never displayed as an actual emotion RICBOT is currently feeling.</li> </ol>"},{"location":"emotion-ui/router/#the-code-below","title":"The code below","text":"<ol> <li><code>arousal</code> and <code>valence</code> are initialized. Relevant for choosing the emotion by values</li> <li>A function to update the values of <code>arousal</code> and <code>valence</code></li> <li>A get function for <code>arousal</code> and <code>valence</code></li> <li>The <code>calculateEmotion()</code> function. The function is more or less a huge switch statement that searches up the current combination of <code>arousal</code> and <code>valence</code> and then sets the emotion accordingly. </li> </ol> <p>As you can see, the basis to continue working on the emotion calculation with arousal and valence already exists. Feel free to use it.</p>"},{"location":"emotion-ui/routes_folder/","title":"The +page.svelte","text":"<p>The actual \"displaying\" happens inside the <code>+page.svelte</code> file inside the routes folder. The <code>+page.svelte</code> can be seen as the homepage or main page of a website. Different subpages, such as settings in this project, are just additional <code>+page.svelte</code> files inside their own folder in the routes folder. To access settings in the browser, just type in http://localhost:5173/settings while running the emotionsystem. The face itself only needs one display page because we just swap the face components in and out based on the current emotion. This saves us the trouble of creating multiple subpages for each emotion. Furthermore, the Websocket connection and the speech input and output from the LLM are also handled here in the main <code>+page.svelte</code>.</p>"},{"location":"emotion-ui/routes_folder/#the-content","title":"The content","text":"<p>First, we import the necessary stuff for the display to function properly. Next, two arrays, one for the allowed emotions and one for the allowed colors, are declared. Lastly, the current emotion in the array is set. The following, really complicated looking code is for establishing, opening and regulating the Websocket that connects the emotionsystem to the LLM via ROS Nodes:</p> <p>1. Declaring the necessary variables 2. Audio playing and receiving functions, which are needed by the LLM. More in \"The Audio Manager\" and \"The Communication with the LLM\" 3. The <code>function reconnect()</code>. It handles the reconnection of the Websocket if the connection is lost. 4. The <code>function connect()</code> establishes the Websocket connection. 5. The <code>ws.onopen</code> sets the Websocket to open and manages the received messages. 6. <code>ws.onmessage</code> which handles the messages for the emotionsystem and the output of the speech 7. The initialization of the microphone and the binding of the face itself 8. Some debug features allowing you to change the emotions and color of the face manually.</p>"},{"location":"emotion-ui/routes_folder/#the-connection-with-llm-emotionsystem","title":"The connection with LLM (Emotionsystem)","text":"<p>The emotionsystem has a built-in subscriber to ROS messages coming from the LLM. Necessary for the displaying of the emotion are which emotion the LLM is currently feeling and if the LLM is currently outputting text. These are handled here:</p> <p><pre><code>\u00a0ws.onmessage = async (event) =&gt; {\n\u00a0 \u00a0 \u00a0 const data = JSON.parse(event.data);\n\u00a0 \u00a0 \u00a0 console.log(data);\n\u00a0 \u00a0 \u00a0 if (data.emotion != undefined) {\n\u00a0 \u00a0 \u00a0 \u00a0 emotion = emotions[data.emotion];\n\u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 \u00a0 if (data.speaking != undefined) {\n\u00a0 \u00a0 \u00a0 \u00a0 speaking = data.speaking;\n\u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 \u00a0 ...\n\u00a0}\n</code></pre> Basically, the LLM sends a ROS Message containing the current emotion of the LLM as an int8 and a boolean if the LLM is currently speaking (output text). The received integer is used inside the <code>setEmotion()</code> function to, surprise, set the emotion, and the boolean is used to update the <code>convesation</code> variable so that the mouth movement can be started or cancelled depending on the received data. Beware that this is just the little part necessary for displaying the emotion. The main part is the audio input and output, which are handled in a different chapter. </p>"},{"location":"emotion-ui/style/","title":"The styling","text":"<p>The whole styling is done in the <code>botface.css</code> inside the lib folder.</p> <p>The script starts with positioning the HTML elements. <code>.facecontainer</code> is the whole <code>div</code> container, everything else are the elements of said container. That\u2019s why there is the <code>svg</code> is at the end of the classes. We displayed them as HTML elements and imported them in a way so we can access the parameters inside the SVG: <code>eye_happy.svg?raw</code></p>"},{"location":"emotion-ui/style/#the-coloring","title":"The coloring","text":"<p>Here is where it gets interesting. First, the colors themselves are stored in a variable so they can be adjusted more easily. Now to the assets: Remember what was written in the documentation for them? Depending on the used coloring parameter and the structure of the SVG, different CSS is used to color them. Let's look at an example:</p> <pre><code>.red path {\n\n\u00a0\u00a0\u00a0 stroke: var(--red);\n\n}\n\n\u00a0\n\n.red circle, .red ellipse, .red rect, .red.strokefill path {\n\n\u00a0\u00a0\u00a0 fill: var(--red);\n\n\u00a0\u00a0\u00a0 stroke: var(--red);\n\n}\n</code></pre> <p>Those are the three CSS paths for coloring the botface red. Which emotion is currently displayed is completely irrelevant due to these three aspects:</p> <p>The first one sets the <code>stroke</code> parameter of all assets with the structure of a path to red.</p> <p>The third one set both the <code>fill</code> and the <code>stroke</code> parameter of all assets who have the structure of a circle, an ellipse, a rectangle or all how have the class <code>stokefill</code> to red.</p> <p>This is the same for the rest of the colors.</p> <p>And thus, the color of the displayed emotion can be changed at any time.</p>"},{"location":"emotion-ui/svelte/","title":"SvelteKit","text":"<p>The web framework SvelteKit was used for programming the emotions of RICBOT.  A web framework is a kind of program that makes the process of writing websites easier. </p> <p>SvelteKit looks overwhelming at first, but it is quite simple and easy to use when you know about its peculiarities.</p>"},{"location":"emotion-ui/svelte/#installation-and-start","title":"Installation and start","text":"<p>SvelteKit has to be installed for each individual project. Like I said, it's a framework and no programming language. If you want to start from scratch (not recommended),  just follow the installation guide on the SvelteKit Website. If you want to continue with the already existing work (recommended), just do the following to access it:  First, clone the <code>emotion</code> repository from GitLab. After doing that, open the repository in Visual Studio Code and type the following commands in the terminal:</p> <p><pre><code>npm install\n</code></pre> This should be enough to install all the dependencies SvelteKit needs to operate on your system.</p> <p>Do this to run the system inside the repository: </p> <p><pre><code>cd CalculateEmotion\nnpm run dev\n</code></pre> Now, the botface will be visible on http://localhost:5173/. </p>"},{"location":"emotion-ui/svelte/#so-many-folders","title":"So many folders ???","text":"<p>Relax, you only have to use a fraction of the displayed folders and components.</p> <p>Let's start with the structure of SvelteKit:</p> <p>Like I said, most of the folders in the editor won't be used by us and can be ignored. The whole project's programmed data, a.k.a., the scrips you will use and work on, is found in SvelteKit's src folder.</p> <p>The src folder contains the following sub-folders and scripts: - lib - routes  - <code>app.d.ts</code> - <code>app.html</code></p>"},{"location":"emotion-ui/svelte/#lib-folder","title":"lib folder","text":"<p>The lib folder contains all scripts and assets used by the application and functions as the \"backend\" for the emotion system.  It contains an <code>assets</code> folder, which stores all the SVG components of the botface, for example, the base of the face or the eyes for the different emotions. Furthermore, it contains the components folder, which holds the <code>botface.svelte</code> and the <code>Slider.svelte</code>. The lib folder also contains the <code>botface.css</code> which regulates the general placement of the base, mouth and eyes on the website. Furthermore, it contains the <code>faceanimation.js</code> which regulates the blinking and speaking animation the botface has when displayed in the browser. Additionally, it also holds the different scripts needed for the audio. To get more information, visit \"The Communication With LLM\" and \"The Audio Manager\". The folder also contains the <code>router.js</code>. This script is where the magic of the emotion calculation happens. It determines which emotion RICBOT is currently feeling and displays the right subpage. <code>botface.css</code>, <code>faceanimation.js</code>, <code>botface.svele</code> and <code>router.js</code> will all be discussed in detail later. </p>"},{"location":"emotion-ui/svelte/#routes-folder","title":"routes folder","text":"<p>The routes folder contains all webpages and is the frontend of the application. For this project, only one page is needed. The <code>index.html</code> so to speak. IF there would be more subpages, they would be stored in here. </p>"},{"location":"emotion-ui/svelte/#the-rest","title":"The rest","text":"<p>The <code>app.d.ts</code> and <code>app.html</code> can be completely ignored and are not relevant for the actual programming. They are used by SvelteKit to host the website.</p>"},{"location":"emotion-ui/svelte/#but-where-is-the-face","title":"But where is the face ???","text":"<p>Like described earlier, type the following in the console of the project: <pre><code>npm run dev\n</code></pre> This opens port 5173, which can be accessed with http://localhost:5173/. 5173 is the default port set by SvelteKit to display it's applications. There you have the face! For development purposes, this is enough. The actual building of the website is done in the Dockerfile. </p> <p>For additional questions regarding SvelteKit, just visit the official SvelteKit website. It has a really good documentation too.</p>"},{"location":"ai/llm/","title":"Welcome to ros_llm","text":"<p>A ROS2 Jazzy client for an OpenAI API compatible LLM server using Llama.CPP as an implementation.</p> <p>The llm node includes an <code>/llm</code> and <code>/clear_history</code> ROS2 service, which are described in the services documentation.</p> <p>This documentation provides all the information you need to get started with <code>ros_llm</code>, from setting up your environment to using the service and contributing to the project.</p> <pre><code>---\nconfig:\n    theme: redux\n    look: neo\n---\nflowchart TB\n    subgraph s1[\"Server\"]\n        direction TB\n        n11[\"Chat Node\"]\n\n        subgraph s11[\"Whisper STT\"]\n            direction TB\n            n112[\"STT Node\"] &lt;-- HTTP --&gt; n111[\"whisper.cpp\"]\n        end\n\n        subgraph s12[\"Gemma 3 12b\"]\n                direction TB\n                n122[\"LLM Node\"] &lt;-- HTTP --&gt; n121[\"llama.cpp\"]\n            end\n\n        subgraph s13[\"Orpheus TTS\"]\n            direction TB\n            n132[\"TTS Node\"] &lt;-- HTTP --&gt; n131[\"llama-swap\"]\n        end\n\n        n11 -- ROS2 --&gt; n112\n        n11 -- ROS2 --&gt; n122\n        n11 -- ROS2 --&gt; n132\n    end\n    subgraph s21[\"Robot\"]\n        direction TB\n        n21[\"UI Com\"] &lt;-- WebSocket --&gt; n22[\"Emotion\"]\n    end\n    n11 &lt;-- \"ROS2\" --&gt; n21</code></pre>"},{"location":"ai/llm/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Visual Studio Code</li> <li>Dev Container Extension</li> </ul>"},{"location":"ai/llm/#contributing","title":"Contributing","text":"<p>We welcome contributions to this project! Please see the contributing guidelines at <code>contributing.md</code> in the root of this repository for more information on how to get started.</p>"},{"location":"ai/llm/code/","title":"Code Documentation","text":"<p>This document provides an overview of the <code>llm_node.py</code> script, which is the core of the <code>ros_llm</code> package.</p>"},{"location":"ai/llm/code/#llamaclientnode","title":"<code>LlamaClientNode</code>","text":"<p>The <code>LlamaClientNode</code> class is a ROS2 node that acts as a client to an OpenAI-compatible LLM server, such as one provided by Llama.cpp, but any other OpenAI-compatible LLM server will suffice.</p> <p>It exposes ROS2 services to interact with the language model.</p>"},{"location":"ai/llm/code/#parameters","title":"Parameters","text":"<p>The node exposes the following ROS2 parameters:</p> Parameter Type Description Default Value <code>server_url</code> string The URL of the Llama.cpp server's chat completions endpoint. <code>http://127.0.0.1:5001/v1/chat/completions</code> <code>temperature</code> double Controls the randomness of the output. Lower values make the output more deterministic, while higher values make it more creative. <code>1.0</code> <code>n_predict</code> integer The maximum number of tokens to generate in the response. A value of <code>-1</code> means no limit. <code>-1</code> <code>history_max_length</code> integer The maximum number of user/assistant conversation turns to keep in the history. <code>10</code> <code>rag_enabled</code> boolean Toggles Retrieval-Augmented Generation (RAG) mode. Note: This feature is currently a placeholder. <code>false</code> <code>rag_system_prompt_template</code> string A template for the system prompt when RAG is enabled. It must contain a <code>{context}</code> placeholder. <code>Use the following context to answer the user's question. If the context doesn't contain the answer, say you don't know.\\n\\n--- Context ---\\n{context}\\n--- End Context ---</code>"},{"location":"ai/llm/code/#system-prompt","title":"System Prompt","text":"<p>The node loads a system prompt from the <code>system_prompt.md</code> file located in the <code>src/llm/resource</code> directory.</p> <p>This prompt is used to set the behavior and personality of the assistant.</p> <p>Note that the current implementation also contains some OrpheusTTS specific tags for emotion inferencing, so those can later be outputted as such in the TTS node.</p>"},{"location":"ai/llm/code/#services","title":"Services","text":"<p>The node provides two services:</p>"},{"location":"ai/llm/code/#llm","title":"<code>/llm</code>","text":"<ul> <li>Type: <code>ric_messages/srv/LLMChat</code></li> <li>Description: This is the main service for interacting with the LLM. It takes a prompt from the user and returns the model's response.</li> <li>Request:<ul> <li><code>prompt</code> (string): The user's input to the model.</li> </ul> </li> <li>Response:<ul> <li><code>response</code> (string): The model's generated text.</li> </ul> </li> </ul>"},{"location":"ai/llm/code/#clear_history","title":"<code>/clear_history</code>","text":"<ul> <li>Type: <code>std_srvs/srv/Empty</code></li> <li>Description: This service clears the conversation history maintained by the node. This is useful for starting a new conversation without restarting the node.</li> <li>Request: (empty)</li> <li>Response: (empty)</li> </ul>"},{"location":"ai/llm/code/#how-it-works","title":"How it Works","text":"<ol> <li>Initialization: The node starts, declares its parameters, loads the system prompt, and creates the services.</li> <li>Service Call: Another ROS2 node calls the <code>/llm</code> service with a prompt.</li> <li>Message Building: The <code>completion_callback</code> is triggered. It calls <code>_build_messages</code> to construct a list of messages, including the system prompt, the conversation history (from a <code>deque</code>), and the new user prompt.</li> <li>API Request: The node sends this list of messages in a JSON payload to the Llama.cpp server via an HTTP POST request.</li> <li>Response Handling:<ul> <li>If the server returns a successful response, the node extracts the generated text.</li> <li>The user prompt and the assistant's response are added to the history deque.</li> <li>The generated text is returned to the service caller.</li> <li>If the server returns an error, an error message is logged and returned.</li> </ul> </li> </ol>"},{"location":"ai/llm/running/","title":"Running with Docker","text":"<p>This project can be run using Docker and Docker Compose. Install it from here if not already available.</p> <p>There are two separate configurations available: one for running with NVIDIA GPU support and another for CPU-only execution.</p> <p>IMPORTANT: Make sure to also clone the <code>ric-messages</code> git submodule located in <code>src</code> folder with:</p> <pre><code>git submodule update --init\n</code></pre>"},{"location":"ai/llm/running/#with-gpu-support","title":"With GPU Support","text":"<p>To run the application with GPU acceleration, you will need to have the NVIDIA Container Toolkit installed on your system.</p> <p>Once you have the toolkit installed, you can run the application using the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will build and run the <code>llm</code> and <code>llm-node</code> services.</p> <p>The <code>llm</code> service will automatically download the specified model and start the Llama.CPP server with GPU support.</p> <p>Important: Do note that the ROS2 node makes use of <code>rmw_zenoh</code> for ROS2 communication. Use the provided zenoh_router for this purpose.</p>"},{"location":"ai/llm/running/#cpu-only","title":"CPU-Only","text":"<p>If you do not have a compatible NVIDIA GPU, you can run the application in CPU-only mode.</p> <p>To do this, use the <code>compose.cpu.yaml</code> file:</p> <pre><code>docker compose -f compose.cpu.yaml up\n</code></pre> <p>This will start the same services, but the <code>llm</code> service will be configured to run entirely on the CPU.</p> <p>Note that the execution time using CPU-only will be very slow.</p>"},{"location":"ai/llm/running/#services","title":"Services","text":"<p>The Docker Compose configurations define two main services: <code>llm</code> and <code>llm-node</code>.</p>"},{"location":"ai/llm/running/#the-llm-service","title":"The <code>llm</code> Service","text":"<p>This service is responsible for running the Llama.CPP server, which provides the core language model inference capabilities.</p> <ul> <li>The <code>llm</code> service uses a pre-built Docker image from <code>ghcr.io/ggml-org/llama.cpp</code> (<code>server-cuda</code> for GPU, <code>server</code> for CPU).</li> <li>It mounts <code>/root/.cache/llama.cpp</code> to <code>.models/llm</code>, so all auto-downloaded models are stored on the local file system and don't need to be re-downloaded when a container is recreated.</li> <li>The server exposes an OpenAI-compatible API endpoint, which the <code>llm-node</code> service communicates with.</li> <li>A healthcheck runs every 30 seconds to ensure the <code>llm-node</code> starts only after the server is running.</li> <li>Check the documentation for llama-server for all available arguments.</li> <li>With the default settings, we use the quantized version of Gemma3 from Unsloth with the recommended settings for Llama.CPP.</li> </ul>"},{"location":"ai/llm/running/#environment","title":"Environment","text":"Variable Description Default Value <code>LLAMACPP_MODEL_NAME</code> The name of the model to download from Hugging Face. <code>unsloth/gemma-3-12b-it-qat-GGUF:Q4_K_M</code> <code>LLAMACPP_CONTEXT_LENGTH</code> The context length of the LLM. <code>16384</code> <code>LLAMACPP_N_GPU_LAYERS</code> The number of layers to offload to the GPU. <code>49</code>"},{"location":"ai/llm/running/#the-llm-node-service","title":"The <code>llm-node</code> Service","text":"<p>This service runs the ROS2 client node that acts as a bridge between the ROS2 ecosystem and the <code>llm</code> service.</p> <ul> <li>Uses <code>harbor.hb.dfki.de/helloric/ros_llm:latest</code> (VPN required) or builds from the local Dockerfile</li> <li>The node provides a ROS2 service at <code>/llm</code> that allows other ROS2 nodes to send prompts and receive completions from the language model.</li> <li>It also offers a <code>/clear_history</code> service to reset the conversation.</li> <li>It communicates with the <code>llm</code> service over the internal Docker network.</li> <li>It is configured to start only after the <code>llm</code> service is healthy and running.</li> <li>It uses Zenoh as RMW implementation by default. To change it, refer to the <code>zenoh_router</code> documentation.</li> </ul>"},{"location":"ai/llm/running/#environment_1","title":"Environment","text":"Variable Description Default Value <code>LLAMACPP_URL</code> URL of the Llama.cpp server. <code>http://llm:8080/v1/chat/completions</code> <code>PYTHONUNBUFFERED</code> Prevents Python from buffering stdout and stderr. <code>1</code> <code>RMW_IMPLEMENTATION</code> ROS2 middleware implementation. <code>rmw_zenoh_cpp</code> <code>ROS_AUTOMATIC_DISCOVERY_RANGE</code> Disables automatic discovery in ROS2. <code>OFF</code> <code>ZENOH_ROUTER_CHECK_ATTEMPTS</code> Number of attempts to check for Zenoh router. <code>0</code> means wait indefinitely. <code>0</code> <code>ZENOH_CONFIG_OVERRIDE</code> Zenoh configuration override, see rmw_zenoh. <code>mode=\"client\";connect/endpoints=[\"tcp/host.docker.internal:7447\"]</code>"},{"location":"ai/llm/service/","title":"Running the Service","text":"<p>This document provides instructions on how to run the <code>ros_llm</code> service node and interact with it.</p>"},{"location":"ai/llm/service/#how-to-run-the-service","title":"How to Run the Service","text":"<p>To run the chat service, you first need to source your ROS2 environment and then use the <code>ros2 run</code> command.</p> <p>You can run the service node with the following command. This will start the LLM client and make it available for receiving prompts.</p> <pre><code># Source your ROS2 workspace\nsource install/setup.bash\n\nros2 run llm llm_node\n</code></pre>"},{"location":"ai/llm/service/#arguments","title":"Arguments","text":"<p>You can customize the behavior of the node by passing the following ROS parameters.</p> <pre><code>ros2 run llm llm_node --ros-args -p &lt;argument&gt;:=&lt;value&gt;\n</code></pre> Argument Description Default <code>server_url</code> The URL of the Llama.CPP server. <code>http://127.0.0.1:5001/v1/chat/completions</code> <code>temperature</code> The sampling temperature. A lower value makes the output more deterministic, while a higher value makes it more creative. <code>1.0</code> <code>n_predict</code> The number of tokens to predict. A smaller value results in shorter answers. Use <code>-1</code> to disable this limit. <code>-1</code> (Disabled) <code>history_max_length</code> The maximum number of previous prompts to keep in the context, defining the \"memory\" of the assistant. <code>10</code> <code>system_prompt</code> The system prompt that guides the assistant's behavior. <code>'You are a helpful robot assistant.'</code> <code>rag_enabled</code> Unimplemented. Toggles Retrieval-Augmented Generation (RAG) mode. <code>false</code> <code>rag_system_prompt_template</code> The system prompt template used for RAG. The <code>{context}</code> placeholder will be replaced with the retrieved information. <code>\"Use the following context to answer the user's question. If the context doesn't contain the answer, say you don't know.\\n\\n--- Context ---\\n{context}\\n--- End Context ---\"</code>"},{"location":"ai/llm/service/#service-requests","title":"Service Requests","text":""},{"location":"ai/llm/service/#llm","title":"LLM","text":"<p>To send a prompt to the LLM, you can call the <code>/llm</code> service.</p> <p>It uses the <code>ric_messages/srv/LLMInput</code> service type. Replace <code>${prompt}</code> with your desired prompt.</p> <pre><code>ros2 service call /llm ric_messages/srv/LLMInput \"{'prompt': '${prompt}'}\"\n</code></pre>"},{"location":"ai/llm/service/#clear-history","title":"Clear History","text":"<p>To clear the conversation history maintained by the node, you can call the <code>/clear_history</code> service. This is useful for starting a fresh conversation without restarting the node.</p> <p>It uses the <code>std_srvs/srv/Empty</code> service type.</p> <pre><code>ros2 service call /clear_history std_srvs/srv/Empty\n</code></pre>"},{"location":"ai/stt/","title":"ROS2 Speech to Text","text":"<p>A ROS2 Jazzy client for an OpenAI API compatible WhisperSTT server using whisper.cpp as an implementation.</p> <p>The speech to text ROS2 node in this solution includes a <code>/stt</code> ROS2 service that accepts audio data and returns a transcribed text and detected language.</p> <p>This documentation provides all the information you need to get started with <code>ros_stt</code>, from setting up your environment to using the service and contributing to the project.</p> <pre><code>---\nconfig:\n    theme: redux\n    look: neo\n---\nflowchart TB\n    subgraph s1[\"Server\"]\n        direction TB\n        n11[\"Chat Node\"]\n\n        subgraph s11[\"Whisper STT\"]\n            direction TB\n            n112[\"STT Node\"] &lt;-- HTTP --&gt; n111[\"whisper.cpp\"]\n        end\n\n        subgraph s12[\"Gemma 3 12b\"]\n                direction TB\n                n122[\"LLM Node\"] &lt;-- HTTP --&gt; n121[\"llama.cpp\"]\n            end\n\n        subgraph s13[\"Orpheus TTS\"]\n            direction TB\n            n132[\"TTS Node\"] &lt;-- HTTP --&gt; n131[\"llama-swap\"]\n        end\n\n        n11 -- ROS2 --&gt; n112\n        n11 -- ROS2 --&gt; n122\n        n11 -- ROS2 --&gt; n132\n    end\n    subgraph s21[\"Robot\"]\n        direction TB\n        n21[\"UI Com\"] &lt;-- WebSocket --&gt; n22[\"Emotion\"]\n    end\n    n11 &lt;-- \"ROS2\" --&gt; n21</code></pre>"},{"location":"ai/stt/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Visual Studio Code</li> <li>Dev Container Extension</li> </ul>"},{"location":"ai/stt/#contributing","title":"Contributing","text":"<p>We welcome contributions to this project! Please see the contributing guidelines at <code>contributing.md</code> in the root of this repository for more information on how to get started.</p>"},{"location":"ai/stt/code/","title":"Code Documentation","text":"<p>This document provides an overview of the <code>stt_node.py</code> script, which is the core of the <code>ros_stt</code> package.</p>"},{"location":"ai/stt/code/#speechtotextnode","title":"<code>SpeechToTextNode</code>","text":"<p>The <code>SpeechToTextNode</code> class is a ROS2 node that acts as a client to an OpenAI-compatible STT server. It exposes a ROS2 service to transcribe audio into text.</p>"},{"location":"ai/stt/code/#parameters","title":"Parameters","text":"<p>The node exposes the following ROS2 parameter:</p> Parameter Type Description Default Value <code>server_url</code> string The URL of the whisper.cpp server endpoint. <code>http://localhost:8080/inference</code>"},{"location":"ai/stt/code/#services","title":"Services","text":"<p>The node provides one main service:</p>"},{"location":"ai/stt/code/#stt","title":"<code>/stt</code>","text":"<ul> <li>Type: <code>ric_messages/srv/AudioBytesToText</code></li> <li>Description: This service takes a raw audio byte array and returns the transcribed text along with the detected language.</li> <li>Request:<ul> <li><code>audio</code> (uint8[]): The raw audio data to be transcribed.</li> </ul> </li> <li>Response:<ul> <li><code>text</code> (string): The transcribed text from the audio.</li> <li><code>language</code> (string): The language automatically detected by the server.</li> </ul> </li> </ul>"},{"location":"ai/stt/code/#how-it-works","title":"How it Works","text":"<ol> <li>Initialization: The node starts, declares its <code>server_url</code> parameter, and creates the <code>/stt</code> service.</li> <li>Service Call: Another ROS2 node calls the <code>/stt</code> service with a request containing the raw audio data as a <code>uint8</code> array.</li> <li>Callback Execution: The <code>speech_to_text_callback</code> method is triggered.</li> <li>Data Preparation: The incoming <code>uint8</code> array is wrapped in an <code>io.BytesIO</code> object to be sent as a file in an HTTP request.</li> <li>API Request: The node sends the audio data in a <code>multipart/form-data</code> POST request to the <code>whisper.cpp</code> server URL. It specifically requests a <code>verbose_json</code> response to ensure it receives the detected language in addition to the text.</li> <li>Response Handling:<ul> <li>If the server returns a successful response (HTTP 200), the node parses the JSON payload.</li> <li>It extracts the <code>text</code> and <code>language</code> fields from the response.</li> <li>The extracted data is populated into the ROS service response object.</li> <li>If the server returns an error or the transcription is empty, an appropriate error or warning is logged.</li> </ul> </li> <li>Return to Caller: The ROS service response, containing the text and language, is returned to the original caller.</li> </ol>"},{"location":"ai/stt/running/","title":"Running with Docker","text":"<p>This project can be run using Docker and Docker Compose. Install it from here if not already available.</p> <p>There are two separate configurations available: one for running with NVIDIA GPU support and another for CPU-only execution.</p> <p>IMPORTANT: Make sure to also clone the <code>ric-messages</code> git submodule located in <code>src</code> folder with:</p> <pre><code>git submodule update --init\n</code></pre>"},{"location":"ai/stt/running/#with-gpu-support","title":"With GPU Support","text":"<p>To run the application with GPU acceleration, you will need to have the NVIDIA Container Toolkit installed on your system.</p> <p>Once you have the toolkit installed, you can run the application using the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will build and run the <code>stt</code> and <code>stt-node</code> services.</p> <p>The <code>stt</code> service will automatically download the specified model and start the Whisper.CPP server with GPU support.</p> <p>Info: If the download script ever fails, that means, the download script from Whisper.CPP in <code>models/download-ggml-model.sh</code> was probably moved or removed. Try to download the <code>ggml-large-v3-turbo-q5_0.bin</code> to <code>/app/models</code>, so it automatically mounts to <code>.models</code>.</p> <p>Important: Do note that the ROS2 node makes use of <code>rmw_zenoh</code> for ROS2 communication. Use the provided zenoh_router for this purpose.</p>"},{"location":"ai/stt/running/#cpu-only","title":"CPU-Only","text":"<p>If you do not have a compatible NVIDIA GPU, you can run the application in CPU-only mode.</p> <p>To do this, use the <code>compose.cpu.yaml</code> file:</p> <pre><code>docker compose -f compose.cpu.yaml up\n</code></pre> <p>This will start the same services, but the <code>stt</code> service will be configured to run entirely on the CPU.</p> <p>Note that the execution time using CPU-only will be much slower than without GPU, but it will be not as slow as an LLM.</p>"},{"location":"ai/stt/running/#services","title":"Services","text":"<p>The Docker Compose configurations define two main services: <code>stt</code> and <code>stt-node</code>, along with a helper service <code>stt-model-downloader</code>.</p>"},{"location":"ai/stt/running/#the-stt-service","title":"The <code>stt</code> Service","text":"<p>This service is responsible for running the <code>whisper.cpp</code> server, which performs the actual speech-to-text transcription.</p> <ul> <li>It is preceded by the <code>stt-model-downloader</code> service, which downloads the specified model from the internet. The model is determined by the <code>WHISPER_MODEL</code> variable in the <code>.env</code> file.</li> <li>The <code>stt</code> service uses a custom Docker image (<code>whisper.cuda.Dockerfile</code> for GPU) or the official <code>whisper.cpp</code> image (<code>compose.cpu.yaml</code> for CPU).</li> <li>It mounts the local <code>./.models</code> directory to <code>/models</code>, so downloaded models are persisted on the host.</li> <li>The server exposes its transcription service on port <code>8080</code> within the Docker network.</li> <li>A healthcheck runs every 30 seconds to ensure the <code>stt-node</code> only starts after the server is running.</li> <li>Check the official Whisper.CPP documentation for all available server arguments.</li> </ul>"},{"location":"ai/stt/running/#environment","title":"Environment","text":"Variable Description Default Value <code>WHISPER_MODEL</code> The name of the model to download. <code>large-v3-turbo-q5_0</code> <code>WHISPER_THREADS</code> The number of threads to use for processing. <code>8</code>"},{"location":"ai/stt/running/#the-stt-node-service","title":"The <code>stt-node</code> Service","text":"<p>This service runs the ROS2 node that acts as a bridge between the ROS2 ecosystem and the <code>stt</code> service.</p> <ul> <li>It builds from the local <code>Dockerfile</code>.</li> <li>The node provides a ROS2 service at <code>/stt</code> that allows other ROS2 nodes to send audio and receive transcribed text.</li> <li>It communicates with the <code>stt</code> service over the internal Docker network.</li> <li>It is configured to start only after the <code>stt</code> service is healthy and running.</li> <li>It uses Zenoh as the RMW implementation by default. To change it, refer to the <code>zenoh_router</code> documentation.</li> </ul>"},{"location":"ai/stt/running/#environment_1","title":"Environment","text":"Variable Description Default Value <code>WHISPER_URL</code> URL of the whisper.cpp server endpoint. <code>http://stt:8080/inference</code> <code>PYTHONUNBUFFERED</code> Prevents Python from buffering stdout and stderr. <code>1</code> <code>RMW_IMPLEMENTATION</code> ROS2 middleware implementation. <code>rmw_zenoh_cpp</code> <code>ROS_AUTOMATIC_DISCOVERY_RANGE</code> Disables automatic discovery in ROS2. <code>OFF</code> <code>ZENOH_ROUTER_CHECK_ATTEMPTS</code> Number of attempts to check for Zenoh router. <code>0</code> means wait indefinitely. <code>0</code> <code>ZENOH_CONFIG_OVERRIDE</code> Zenoh configuration override, see rmw_zenoh. <code>mode=\"client\";connect/endpoints=[\"tcp/host.docker.internal:7447\"]</code>"},{"location":"ai/stt/running/#usage","title":"Usage","text":"<p>Create a ROS2 client for the <code>/stt</code> service and call it. The service uses the <code>ric_messages/srv/AudioBytesToText</code> interface. For exact definition check out the <code>ric_messages</code> repository. For usage examples, check out service.</p>"},{"location":"ai/stt/service/","title":"Running the Service","text":"<p>This document provides instructions on how to run the <code>ros_stt</code> service node and interact with it.</p>"},{"location":"ai/stt/service/#how-to-run-the-service","title":"How to Run the Service","text":"<p>To run the chat service, you first need to source your ROS2 environment and then use the <code>ros2 run</code> command.</p> <p>You can run the service node with the following command. This will start the STT client and make it available for receiving prompts.</p> <pre><code># Source your ROS2 workspace\nsource install/setup.bash\n\nros2 run stt service\n</code></pre> <p>When running the service in docker, you can enter the container with the following command, where the above steps are already done:</p> <pre><code>docker exec -it stt-node bash\n</code></pre>"},{"location":"ai/stt/service/#arguments","title":"Arguments","text":"<p>You can customize the behavior of the node by passing the following ROS parameters.</p> <pre><code>ros2 run stt service --ros-args -p &lt;argument&gt;:=&lt;value&gt;\n</code></pre> Argument Description Default Value <code>server_url</code> The URL of the whisper.cpp server endpoint. <code>http://localhost:8080/inference</code>"},{"location":"ai/stt/service/#service-definition","title":"Service Definition","text":""},{"location":"ai/stt/service/#stt","title":"<code>/stt</code>","text":"<ul> <li>Type: <code>ric_messages/srv/AudioBytesToText</code></li> <li>Description: Takes an audio input and returns the transcribed text and detected language.</li> <li>Request: <code>uint8[] audio</code></li> <li>Response: <code>string text</code>, <code>string language</code></li> </ul> <p>To call the service from the command line, you need to provide the audio as an array of bytes.</p> <pre><code>ros2 service call /stt ric_messages/srv/AudioBytesToText \"{'audio': [0, 1, 2, ...]}\"\n</code></pre> <p>Note: Providing a raw audio byte array via the command line is impractical for real audio files. This method is primarily for testing with very short or empty audio clips. For actual use, it is recommended to create a dedicated ROS2 client node in Python or C++ that can read an audio file and send the request. The <code>convert.py</code> script can be used as an example of how to convert a <code>.wav</code> file into a JSON with the required byte array format. To use the JSON, use the following command:</p> <pre><code>ros2 service call /stt ric_messages/srv/AudioBytesToText --stdin &lt; encoded_audio.json\n</code></pre>"},{"location":"ai/stt/service/#response-example","title":"Response Example","text":"<p>A successful call will return a response object containing the transcribed text and language:</p> <pre><code>response:\nric_messages.srv.AudioBytesToText_Response(\n    text='Hello world.',\n    language='en'\n)\n</code></pre>"},{"location":"ai/tts/","title":"ROS2 Text to Speech","text":"<p>A ROS2 Jazzy client for an OpenAI API compatible TTS server using OrpheusTTS as an implementation.</p> <p>The TTS models for specific languages are accessed through llama-swap that runs them in instances of <code>llama-server</code> from llama.cpp. Currently there is support only for German and English.</p> <p>For decoding the response tokens from the TTS model into actual audio the solution uses SNAC (Multi-Scale Neural Audio Codec). This part might be unneeded in the future as llama.cpp should get support for TTS models.</p> <p>The tts node includes <code>/tts</code> as a ROS2 service, which is described in the services documentation.</p> <p>This documentation provides all the information you need to get started with <code>ros_tts</code>, from setting up your environment to using the service and contributing to the project.</p> <pre><code>---\nconfig:\n    theme: redux\n    look: neo\n---\nflowchart TB\n    subgraph s1[\"Server\"]\n        direction TB\n        n11[\"Chat Node\"]\n\n        subgraph s11[\"Whisper STT\"]\n            direction TB\n            n112[\"STT Node\"] &lt;-- HTTP --&gt; n111[\"whisper.cpp\"]\n        end\n\n        subgraph s12[\"Gemma 3 12b\"]\n                direction TB\n                n122[\"LLM Node\"] &lt;-- HTTP --&gt; n121[\"llama.cpp\"]\n            end\n\n        subgraph s13[\"Orpheus TTS\"]\n            direction TB\n            n132[\"TTS Node\"] &lt;-- HTTP --&gt; n131[\"llama-swap\"]\n        end\n\n        n11 -- ROS2 --&gt; n112\n        n11 -- ROS2 --&gt; n122\n        n11 -- ROS2 --&gt; n132\n    end\n    subgraph s21[\"Robot\"]\n        direction TB\n        n21[\"UI Com\"] &lt;-- WebSocket --&gt; n22[\"Emotion\"]\n    end\n    n11 &lt;-- \"ROS2\" --&gt; n21</code></pre>"},{"location":"ai/tts/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Visual Studio Code</li> <li>Dev Container Extension</li> </ul>"},{"location":"ai/tts/#contributing","title":"Contributing","text":"<p>We welcome contributions to this project! Please see the contributing guidelines at <code>contributing.md</code> in the root of this repository for more information on how to get started.</p>"},{"location":"ai/tts/code/","title":"Code Documentation","text":"<p>This document provides an overview of the code of the <code>tts</code> package.</p>"},{"location":"ai/tts/code/#texttospeechnode","title":"<code>TextToSpeechNode</code>","text":"<p>The <code>TextToSpeechNode</code> class is a ROS2 node that acts as a client to an OpenAI-compatible TTS server. We use OrpheusTTS, which models are in the GGUF format, so they are usable in Llama.CPP, which is usually used for LLMs, but we can also use Llama.CPP as an TTS server instead. Note: The node leverages the SNAC (Scalable Neural Audio Codec) model for audio decoding and supports streaming audio generation for real-time text-to-speech conversion, since Llama.CPP doesn't support TTS natively.</p>"},{"location":"ai/tts/code/#parameters","title":"Parameters","text":"<p>The node exposes the following ROS2 parameters: </p> Parameter Type Description Default Value <code>server_url</code> string The URL of the Llama.CCP server's completions endpoint for TTS inference. <code>http://localhost:8080/v1/completions</code> <code>en_model</code> string Model identifier for English TTS. <code>en</code> <code>en_voice</code> string Voice profile to use for English text-to-speech. <code>leah</code> <code>en_max_tokens</code> integer Maximum number of tokens to generate for English TTS. <code>10240</code> <code>en_temperature</code> double Controls randomness in English TTS generation. Higher values increase creativity. <code>0.6</code> <code>en_top_p</code> double Nucleus sampling parameter for English TTS. Controls diversity of token selection. <code>0.9</code> <code>en_repeat_penalty</code> double Penalty for token repetition in English TTS to encourage more varied output. <code>1.1</code> <code>de_model</code> string Model identifier for German TTS. <code>de</code> <code>de_voice</code> string Voice profile to use for German text-to-speech. <code>max</code> <code>de_max_tokens</code> integer Maximum number of tokens to generate for German TTS. <code>10240</code> <code>de_temperature</code> double Controls randomness in German TTS generation. Higher values increase creativity. <code>0.6</code> <code>de_top_p</code> double Nucleus sampling parameter for German TTS. Controls diversity of token selection. <code>0.9</code> <code>de_repeat_penalty</code> double Penalty for token repetition in German TTS to encourage more varied output. <code>1.1</code>"},{"location":"ai/tts/code/#snac-model-initialization","title":"SNAC Model Initialization","text":"<p>The node initializes the SNAC (Scalable Neural Audio Codec) model during startup. SNAC is used to decode the audio tokens generated by the TTS model into raw audio data. The model automatically selects CUDA if available, otherwise falls back to CPU processing.</p>"},{"location":"ai/tts/code/#services","title":"Services","text":"<p>The node provides one main service:</p>"},{"location":"ai/tts/code/#tts","title":"<code>/tts</code>","text":"<ul> <li>Type: <code>ric_messages/srv/TextToAudioBytes</code></li> <li>Description: This is the main service for converting text to audio. It takes text input and a language specification, then returns the generated audio as WAV-formatted bytes.</li> <li>Request:<ul> <li><code>text</code> (string): The text to convert to speech.</li> <li><code>language</code> (string): The target language for synthesis. Supports \"english\"/\"en\" and \"german\"/\"de\".</li> </ul> </li> <li>Response:<ul> <li><code>audio</code> (bytes): The generated audio data in WAV format, ready for playback or further processing.</li> </ul> </li> </ul>"},{"location":"ai/tts/code/#how-it-works","title":"How it Works","text":"<ol> <li> <p>Initialization: The node starts, declares its parameters for both English and German TTS, initializes the SNAC model, and creates the TTS service.</p> </li> <li> <p>Service Call: Another ROS2 node calls the <code>/tts</code> service with text and language parameters.</p> </li> <li> <p>Language Processing: The <code>text_to_speech_callback</code> is triggered. It normalizes the language parameter (converting \"english\" to \"en\" and \"german\" to \"de\") and validates that the language is supported.</p> </li> <li> <p>Parameter Retrieval: The node retrieves the appropriate model parameters based on the requested language (model name, voice, temperature, etc.).</p> </li> <li> <p>Prompt Building: The text is formatted using the <code>build_prompt</code> helper function, which wraps the input text with the appropriate voice tags and special tokens required by the OrpheusTTS model.</p> </li> <li> <p>Streaming Generation: The node sends a streaming request to the Llama.CPP server via <code>_generate_response()</code>:</p> </li> <li>Sends an HTTP POST request with the formatted prompt and generation parameters</li> <li>Processes the server-sent events (SSE) stream response</li> <li> <p>Filters the response to extract only tokens containing audio data (custom tokens)</p> </li> <li> <p>Real-time Audio Decoding: As audio tokens are generated:</p> </li> <li>The <code>tokens_decoder_sync</code> function processes the token stream</li> <li>Tokens are converted to audio codes and passed to the SNAC model</li> <li>The SNAC model decodes the codes into raw audio samples</li> <li> <p>Audio samples are converted to 16-bit PCM format and yielded as byte chunks</p> </li> <li> <p>WAV File Assembly: </p> </li> <li>A WAV header is created using <code>create_wav_header()</code></li> <li>Audio byte chunks are collected and combined with the header</li> <li> <p>The complete WAV file is returned as the service response</p> </li> <li> <p>Error Handling: The node includes comprehensive error handling for network issues, invalid responses, unsupported languages, and audio generation failures.</p> </li> </ol>"},{"location":"ai/tts/code/#key-features","title":"Key Features","text":"<ul> <li>Streaming Audio Generation: Audio is generated and returned in real-time as tokens are produced, enabling low-latency TTS.</li> <li>Multi-language Support: Supports both English and German with separate parameter sets for each language, but other available languages in OrpheusTTS can be used as well.</li> <li>Flexible Voice Selection: Different voice profiles can be configured for each language.</li> <li>SNAC Audio Decoding: Uses state-of-the-art neural audio codec for high-quality audio synthesis.</li> <li>WAV Format Output: Returns standard WAV-formatted audio compatible with most audio systems.</li> <li>Robust Error Handling: Comprehensive error checking and logging throughout the pipeline.</li> </ul>"},{"location":"ai/tts/code/#helper-module-helperpy","title":"Helper Module (<code>helper.py</code>)","text":"<p>The helper module provides essential utility functions for the TTS system, handling prompt formatting, WAV file creation, and token validation.</p>"},{"location":"ai/tts/code/#functions","title":"Functions","text":""},{"location":"ai/tts/code/#string_contains_tokenstring-str-bool","title":"<code>string_contains_token(string: str) -&gt; bool</code>","text":"<ul> <li>Description: Checks if a string contains any custom audio token using regex pattern matching.</li> <li>Parameters: </li> <li><code>string</code> (str): The input string to check for custom tokens</li> <li>Returns: <code>bool</code> - True if the string contains custom tokens, False otherwise</li> <li>Usage: Used to filter streaming responses and identify chunks containing audio data</li> </ul>"},{"location":"ai/tts/code/#build_promptvoice-str-prompt-str-str","title":"<code>build_prompt(voice: str, prompt: str) -&gt; str</code>","text":"<ul> <li>Description: Constructs the properly formatted prompt string required by the OrpheusTTS model, wrapping the input text with voice tags and special tokens.</li> <li>Parameters:</li> <li><code>voice</code> (str): The voice profile to use (e.g., \"leah\", \"max\")</li> <li><code>prompt</code> (str): The text content to be converted to speech</li> <li>Returns: <code>str</code> - The formatted prompt string with OrpheusTTS-specific tokens</li> <li>Format: <code>&lt;custom_token_3&gt;{voice}: {prompt}&lt;|eot_id|&gt;&lt;custom_token_4&gt;&lt;custom_token_5&gt;&lt;custom_token_1&gt;</code></li> </ul>"},{"location":"ai/tts/code/#create_wav_headersample_rate24000-bits_per_sample16-channels1","title":"<code>create_wav_header(sample_rate=24000, bits_per_sample=16, channels=1)</code>","text":"<ul> <li>Description: Creates a standard WAV file header with the specified audio parameters. This function is adapted from the OrpheusTTS project.</li> <li>Parameters:</li> <li><code>sample_rate</code> (int): Audio sample rate in Hz (default: 24000)</li> <li><code>bits_per_sample</code> (int): Bit depth of audio samples (default: 16)</li> <li><code>channels</code> (int): Number of audio channels (default: 1 for mono)</li> <li>Returns: <code>bytes</code> - The WAV header as a byte string</li> <li>Technical Details: Uses struct.pack to create a proper RIFF/WAVE header format</li> </ul>"},{"location":"ai/tts/code/#constants","title":"Constants","text":"<ul> <li><code>AUDIO_TOKENS_REGEX</code>: Regular expression pattern <code>r\"&lt;custom_token_(\\d+)&gt;\"</code> used to identify custom audio tokens in the streaming response</li> </ul>"},{"location":"ai/tts/code/#decoder-module-decoderpy","title":"Decoder Module (<code>decoder.py</code>)","text":"<p>The decoder module handles the conversion of TTS model tokens into actual audio using the SNAC (Scalable Neural Audio Codec) model.</p> <p>This module is adapted from the OrpheusTTS project and serves as a temporary solution until Llama.CPP gains native TTS support.</p>"},{"location":"ai/tts/code/#global-variables","title":"Global Variables","text":"<ul> <li><code>model</code>: The global SNAC model instance used for audio decoding</li> <li><code>snac_device</code>: The device (CPU/CUDA) where the SNAC model is loaded</li> </ul>"},{"location":"ai/tts/code/#functions_1","title":"Functions","text":""},{"location":"ai/tts/code/#initialize_snac_model","title":"<code>initialize_snac_model()</code>","text":"<ul> <li>Description: Initializes the global SNAC model for audio decoding. Automatically detects and uses CUDA if available, otherwise falls back to CPU.</li> <li>Device Selection: Uses the <code>SNAC_DEVICE</code> environment variable or auto-detects the best available device</li> <li>Model: Loads the pre-trained SNAC model from \"hubertsiuzdak/snac_24khz\"</li> </ul>"},{"location":"ai/tts/code/#convert_to_audiomultiframe","title":"<code>convert_to_audio(multiframe)</code>","text":"<ul> <li>Description: Converts a sequence of audio codes into raw audio bytes using the SNAC model.</li> <li>Parameters:</li> <li><code>multiframe</code>: List of audio codes representing frames to be decoded</li> <li>Returns: <code>bytes</code> - Raw audio data in 16-bit PCM format, or None if conversion fails</li> <li>Process:</li> <li>Validates that the multiframe contains at least 7 codes</li> <li>Organizes codes into three hierarchical levels (codes_0, codes_1, codes_2)</li> <li>Performs bounds checking to ensure all codes are within valid range (0-4096)</li> <li>Uses SNAC model to decode codes into audio waveform</li> <li>Converts float audio to 16-bit integer format and returns as bytes</li> </ul>"},{"location":"ai/tts/code/#turn_token_into_idtoken_string-index","title":"<code>turn_token_into_id(token_string, index)</code>","text":"<ul> <li>Description: Extracts and converts custom tokens from the streaming response into audio code IDs.</li> <li>Parameters:</li> <li><code>token_string</code> (str): String containing the custom token</li> <li><code>index</code> (int): Current position in the token sequence</li> <li>Returns: <code>int</code> - The audio code ID, or None if parsing fails</li> <li>Logic: Extracts the numeric part from custom tokens and applies mathematical transformation based on the index position</li> </ul>"},{"location":"ai/tts/code/#tokens_decodertoken_gen-async","title":"<code>tokens_decoder(token_gen)</code> (Async)","text":"<ul> <li>Description: Asynchronous generator that processes a stream of tokens and yields audio chunks in real-time.</li> <li>Parameters:</li> <li><code>token_gen</code>: Async generator yielding token strings</li> <li>Yields: <code>bytes</code> - Audio data chunks as they become available</li> <li>Buffering Strategy: </li> <li>Maintains a buffer of audio codes</li> <li>Processes codes in groups of 7 (representing one audio frame)</li> <li>Yields audio when buffer contains at least 28 codes (4 frames)</li> <li>Uses overlapping windows for smooth audio generation</li> </ul>"},{"location":"ai/tts/code/#tokens_decoder_syncsyn_token_gen","title":"<code>tokens_decoder_sync(syn_token_gen)</code>","text":"<ul> <li>Description: Synchronous wrapper around the async <code>tokens_decoder</code> function, enabling integration with synchronous code.</li> <li>Parameters:</li> <li><code>syn_token_gen</code>: Synchronous generator yielding token strings</li> <li>Yields: <code>bytes</code> - Audio data chunks</li> <li>Implementation:</li> <li>Converts synchronous generator to async generator</li> <li>Runs async decoder in a separate thread</li> <li>Uses a queue to bridge async and sync worlds</li> <li>Returns audio chunks as they become available</li> </ul>"},{"location":"ai/tts/running/","title":"Running with Docker","text":"<p>This project can be run using Docker and Docker Compose. Install it from here if not already available.</p> <p>There are two separate configurations available: one for running with NVIDIA GPU support and another for CPU-only execution.</p> <p>IMPORTANT: Make sure to also clone the <code>ric-messages</code> git submodule located in <code>src</code> folder with:</p> <pre><code>git submodule update --init\n</code></pre>"},{"location":"ai/tts/running/#with-gpu-support","title":"With GPU Support","text":"<p>To run the application with GPU acceleration, you will need to have the NVIDIA Container Toolkit installed on your system.</p> <p>Once you have the toolkit installed, you can run the application using the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will build and run the <code>tts</code> and <code>tts-node</code> services.</p> <p>The <code>tts</code> service will automatically download the specified TTS models and start the llama-swap server with GPU support.</p> <p>Important: Do note that the ROS2 node makes use of <code>rmw_zenoh</code> for ROS2 communication. Use the provided zenoh_router for this purpose.</p>"},{"location":"ai/tts/running/#cpu-only","title":"CPU-Only","text":"<p>If you do not have a compatible NVIDIA GPU, you can run the application in CPU-only mode.</p> <p>To do this, use the <code>compose.cpu.yaml</code> file:</p> <pre><code>docker compose -f compose.cpu.yaml up\n</code></pre> <p>This will start the same services, but the <code>tts</code> service will be configured to run entirely on the CPU.</p> <p>Note that the execution time using CPU-only will be very slow.</p>"},{"location":"ai/tts/running/#services","title":"Services","text":"<p>The Docker Compose configurations define three main services: <code>tts-model-downloader</code>, <code>tts</code>, and <code>tts-node</code>.</p>"},{"location":"ai/tts/running/#the-tts-model-downloader-service","title":"The <code>tts-model-downloader</code> Service","text":"<p>This service is responsible for downloading the required Orpheus TTS models from Hugging Face.</p> <ul> <li>Uses a lightweight Python Alpine image to install <code>huggingface_hub[cli]</code></li> <li>Downloads two models:</li> <li>English model: <code>isaiahbjork/orpheus-3b-0.1-ft-Q4_K_M-GGUF</code></li> <li>German model: <code>TheVisitorX/3b-de-ft-research_release-Q4_K_M-GGUF</code></li> <li>Models are stored in <code>./.models</code> directory on the host system</li> <li>Runs as an initialization step before other services start</li> </ul>"},{"location":"ai/tts/running/#the-tts-service","title":"The <code>tts</code> Service","text":"<p>This service runs the llama-swap server, which manages the TTS model instances and provides an OpenAI-compatible API endpoint.</p> <ul> <li>Uses pre-built Docker images from <code>ghcr.io/mostlygeek/llama-swap</code> (<code>cuda</code> for GPU, <code>cpu</code> for CPU-only)</li> <li>Manages multiple TTS models through llama-swap configuration</li> <li>Uses <code>llama-swap.multi.config.yaml</code> for multi-model support</li> <li>Exposes port 8080 internally for the TTS API</li> <li>Includes health checks to ensure proper startup sequence</li> </ul>"},{"location":"ai/tts/running/#environment-variables","title":"Environment Variables","text":"Variable Description Default Value <code>LLAMA_ARG_N_PARALLEL</code> Number of requests to process in parallel <code>2</code> <code>LLAMA_ARG_THREADS</code> Number of threads to use (-1 for all available) <code>-1</code> <code>LLAMA_ARG_N_GPU_LAYERS</code> Number of model layers to offload to GPU <code>49</code> <code>LLAMA_ARG_NO_WEBUI</code> Disable the web interface <code>true</code>"},{"location":"ai/tts/running/#the-tts-node-service","title":"The <code>tts-node</code> Service","text":"<p>This service runs the ROS2 client node that acts as a bridge between the ROS2 ecosystem and the <code>tts</code> service.</p> <ul> <li>Uses <code>harbor.hb.dfki.de/helloric/ros_tts:latest</code> (VPN required) or builds from the local Dockerfile</li> <li>The node provides a ROS2 service at <code>/tts</code> that allows other ROS2 nodes to send text and receive audio</li> <li>Supports both English and German text-to-speech conversion</li> <li>Communicates with the <code>tts</code> service over the internal Docker network</li> <li>Configured to start only after the <code>tts</code> service is healthy and running</li> <li>Uses Zenoh as RMW implementation by default</li> </ul>"},{"location":"ai/tts/running/#environment-variables_1","title":"Environment Variables","text":"Variable Description Default Value <code>LLAMACPP_URL</code> URL of the llama-swap server's completions endpoint <code>http://tts:8080/v1/completions</code> <code>PYTHONUNBUFFERED</code> Prevents Python from buffering stdout and stderr <code>1</code> <code>RMW_IMPLEMENTATION</code> ROS2 middleware implementation <code>rmw_zenoh_cpp</code> <code>ROS_AUTOMATIC_DISCOVERY_RANGE</code> Disables automatic discovery in ROS2 <code>OFF</code> <code>ZENOH_ROUTER_CHECK_ATTEMPTS</code> Number of attempts to check for Zenoh router. <code>0</code> means wait indefinitely <code>0</code> <code>ZENOH_CONFIG_OVERRIDE</code> Zenoh configuration override, see rmw_zenoh <code>mode=\"client\";connect/endpoints=[\"tcp/host.docker.internal:7447\"]</code>"},{"location":"ai/tts/running/#usage","title":"Usage","text":"<p>Create a ROS2 client for the <code>/tts</code> service and call it. The service uses the <code>ric_messages/srv/TextToAudioBytes</code> interface. For exact definition check out the <code>ric_messages</code> repository. For usage examples, check out service.</p>"},{"location":"ai/tts/service/","title":"Running the Service","text":"<p>This document provides instructions on how to run the <code>tts</code> service node and interact with it.</p>"},{"location":"ai/tts/service/#how-to-run-the-service","title":"How to Run the Service","text":"<p>To run the text-to-speech service, you first need to source your ROS2 environment and then use the <code>ros2 run</code> command.</p> <p>You can run the service node with the following command. This will start the TTS client and make it available for receiving text-to-speech requests.</p> <pre><code># Source your ROS2 workspace\nsource install/setup.bash\n\nros2 run tts service\n</code></pre> <p>When running the service in docker, you can enter the container with the following command, where the above steps are already done:</p> <pre><code>docker exec -it tts-node bash\n</code></pre> <p>With that, you should be able to follow the next instructions.</p>"},{"location":"ai/tts/service/#parameters","title":"Parameters","text":"<p>You can customize the behavior of the node by passing the following ROS parameters.</p> <pre><code>ros2 run tts service --ros-args -p &lt;argument&gt;:=&lt;value&gt;\n</code></pre> Argument Description Default <code>server_url</code> The URL of the llama.cpp server's completions endpoint for TTS inference. <code>http://localhost:8080/v1/completions</code> <code>en_model</code> Model identifier for English TTS. <code>en</code> <code>en_voice</code> Voice profile to use for English text-to-speech. <code>leah</code> <code>en_max_tokens</code> Maximum number of tokens to generate for English TTS. <code>10240</code> <code>en_temperature</code> Controls randomness in English TTS generation. Higher values increase creativity. <code>0.6</code> <code>en_top_p</code> Nucleus sampling parameter for English TTS. Controls diversity of token selection. <code>0.9</code> <code>en_repeat_penalty</code> Penalty for token repetition in English TTS to encourage more varied output. <code>1.1</code> <code>de_model</code> Model identifier for German TTS. <code>de</code> <code>de_voice</code> Voice profile to use for German text-to-speech. <code>max</code> <code>de_max_tokens</code> Maximum number of tokens to generate for German TTS. <code>10240</code> <code>de_temperature</code> Controls randomness in German TTS generation. Higher values increase creativity. <code>0.6</code> <code>de_top_p</code> Nucleus sampling parameter for German TTS. Controls diversity of token selection. <code>0.9</code> <code>de_repeat_penalty</code> Penalty for token repetition in German TTS to encourage more varied output. <code>1.1</code>"},{"location":"ai/tts/service/#service-requests","title":"Service Requests","text":""},{"location":"ai/tts/service/#tts","title":"<code>/tts</code>","text":"<p>To convert text to speech, you can call the <code>/tts</code> service.</p> <p>It uses the <code>ric_messages/srv/TextToAudioBytes</code> service type. Replace <code>${text}</code> with your desired text and <code>${language}</code> with the target language.</p> <pre><code>ros2 service call /tts ric_messages/srv/TextToAudioBytes \"{'text': '${text}', 'language': '${language}'}\"\n</code></pre>"},{"location":"ai/tts/service/#supported-languages","title":"Supported Languages","text":"<p>The service currently supports the following languages:</p> <ul> <li>English: Use <code>\"english\"</code> or <code>\"en\"</code></li> <li>German: Use <code>\"german\"</code> or <code>\"de\"</code></li> </ul>"},{"location":"ai/tts/service/#example-usage","title":"Example Usage","text":"<p>Convert English text to speech: <pre><code>ros2 service call /tts ric_messages/srv/TextToAudioBytes \"{'text': 'Hello, how are you today?', 'language': 'english'}\"\n</code></pre></p> <p>Convert German text to speech: <pre><code>ros2 service call /tts ric_messages/srv/TextToAudioBytes \"{'text': 'Hallo, wie geht es dir heute?', 'language': 'german'}\"\n</code></pre></p>"},{"location":"ai/tts/service/#response","title":"Response","text":"<p>The service returns audio data in WAV format as a byte array in the <code>audio</code> field of the response. This audio can be saved to a file or played directly by audio processing applications. You can use the <code>convert.py</code> script as an example to save the audio response to a WAV file. First, save the service response to a JSON file, then convert it:</p> <pre><code># Call the service and save response to JSON\nros2 service call /tts ric_messages/srv/TextToAudioBytes \"{'text': 'Hello world', 'language': 'english'}\" &gt; input.json\n\n# Convert the JSON response to a WAV file\npython3 convert.py\n</code></pre> <p>The <code>convert.py</code> script reads the JSON response, extracts the audio byte array, and saves it as <code>output.wav</code>:</p> <p>After running the script, you can play the generated <code>output.wav</code> file with any audio player.</p>"},{"location":"ai/tts/service/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ai/tts/service/#common-issues","title":"Common Issues","text":"<ol> <li>Service not available: Ensure the TTS server is running and accessible at the configured URL</li> <li>Unsupported language: Check that you're using a supported language identifier (\"en\"/\"english\" or \"de\"/\"german\")</li> <li>Empty audio response: Verify that the input text is not empty and the model parameters are correctly configured</li> <li>GPU out of memory: If using GPU deployment, consider reducing <code>LLAMA_ARG_N_GPU_LAYERS</code> or switch to CPU deployment</li> </ol>"},{"location":"ai/tts/service/#checking-service-status","title":"Checking Service Status","text":"<p>To verify the service is running: <pre><code>ros2 service list | grep tts\n</code></pre></p> <p>To check service type: <pre><code>ros2 service type /tts\n</code></pre></p>"},{"location":"ai/tts/service/#logs","title":"Logs","text":"<p>View service logs when running in Docker: <pre><code>docker logs tts-node\n</code></pre></p>"},{"location":"ai/chat/","title":"ROS2 Chat","text":"<p>A wrapper ROS2 node for ros_llm, ros_stt and ros_tts to simplify the chat pipeline.</p> <p>The chat node in this solution includes a <code>/chat</code> ROS2 service that accepts audio data and returns a generated audio response with extra metadata.</p> <p>This documentation provides all the information you need to get started with <code>ros_chat</code>, from setting up your environment to using the service and contributing to the project.</p> <pre><code>---\nconfig:\n    theme: redux\n    look: neo\n---\nflowchart TB\n    subgraph s1[\"Server\"]\n        direction TB\n        n11[\"Chat Node\"]\n\n        subgraph s11[\"Whisper STT\"]\n            direction TB\n            n112[\"STT Node\"] &lt;-- HTTP --&gt; n111[\"whisper.cpp\"]\n        end\n\n        subgraph s12[\"Gemma 3 12b\"]\n                direction TB\n                n122[\"LLM Node\"] &lt;-- HTTP --&gt; n121[\"llama.cpp\"]\n            end\n\n        subgraph s13[\"Orpheus TTS\"]\n            direction TB\n            n132[\"TTS Node\"] &lt;-- HTTP --&gt; n131[\"llama-swap\"]\n        end\n\n        n11 -- ROS2 --&gt; n112\n        n11 -- ROS2 --&gt; n122\n        n11 -- ROS2 --&gt; n132\n    end\n    subgraph s21[\"Robot\"]\n        direction TB\n        n21[\"UI Com\"] &lt;-- WebSocket --&gt; n22[\"Emotion\"]\n    end\n    n11 &lt;-- \"ROS2\" --&gt; n21</code></pre>"},{"location":"ai/chat/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Visual Studio Code</li> <li>Dev Container Extension</li> </ul>"},{"location":"ai/chat/#contributing","title":"Contributing","text":"<p>We welcome contributions to this project! Please see the contributing guidelines at <code>contributing.md</code> in the root of this repository for more information on how to get started.</p>"},{"location":"ai/chat/code/","title":"Code documentation","text":"<p>The <code>chat_node</code> is a central component that orchestrates the entire chat workflow. It integrates multiple services to process an audio input and generate an audio response from it.</p> <pre><code>graph TD\n    subgraph User\n        A[Client]\n    end\n\n    subgraph \"ROS2 Services\"\n        B(chat_node)\n        C(STT Service /stt)\n        D(LLM Service /llm)\n        E(TTS Service /tts)\n    end\n\n    A -- \"UI Audio\" --&gt; B\n    B -- \"UI Audio\" --&gt; C\n    C -- \"Transcription + Language\" --&gt; B\n    B -- \"Transcription\" --&gt; D\n    D -- \"Response\" --&gt; B\n    B -- \"Response + Language\" --&gt; E\n    E -- \"Response Audio\" --&gt; B\n    B -- \"Response Audio + Response + Language + Emotion\" --&gt; A</code></pre> <p>The <code>chat_node</code> performs the following steps:</p> <ol> <li>Receives an audio request: It listens on the <code>/chat</code> service (<code>ric_messages/srv/Chat</code>) for an incoming request containing audio data.</li> <li>Speech-to-Text (STT): It calls the <code>/stt</code> service (<code>ric_messages/srv/AudioBytesToText</code>) to transcribe the input audio into text and detect the language (supports English and German).</li> <li>Language Model (LLM): It sends the transcribed text to the <code>/llm</code> service (<code>ric_messages/srv/LLMChat</code>) to generate a meaningful and context-aware response. The node also extracts an emotion from the LLM's response (e.g., <code>{calm}</code>).</li> <li>Text-to-Speech (TTS): It takes the cleaned text response from the LLM and calls the <code>/tts</code> service (<code>ric_messages/srv/TextToAudioBytes</code>) to convert it back into audio.</li> <li>Returns Response: It sends the final generated audio, language, text, and emotion back to the original caller.</li> </ol>"},{"location":"ai/chat/code/#asynchronous-processing","title":"Asynchronous Processing","text":"<p>The <code>chat_node</code> is designed to be highly responsive and efficient, leveraging asynchronous programming to handle long-running tasks without blocking the main execution thread. This is crucial for a robotics system where nodes must remain responsive to other events.</p> <ul> <li> <p>The node uses Python's <code>asyncio</code> library to manage concurrent operations. The <code>chat_callback</code> and the service client calls (<code>_send_stt_request</code>, <code>_send_llm_request</code>, <code>_send_tts_request</code>) are defined as <code>async</code> functions. This allows the node to <code>await</code> the results of the service calls (which can take a significant amount of time) without freezing. While waiting for a response from one service, the node can still process other events.</p> </li> <li> <p>The <code>/chat</code> service is created with a <code>ReentrantCallbackGroup</code>. It allows the <code>chat_callback</code> to be called again while a previous call is still <code>await</code>-ing a long-running operation (like a call to the LLM service). This prevents deadlocks and allows the node to handle multiple concurrent requests to the <code>/chat</code> service. That means multiple calls to the <code>/chat</code> service, while the service is still processing the previous request won't \"eat up\" the request and would queue up the request instead.</p> </li> <li> <p>The <code>main</code> function and <code>spin_node</code> function integrate the <code>asyncio</code> event loop with the ROS2 event system (<code>rclpy</code>). The <code>spin_node</code> function uses <code>rclpy.spin_once()</code> within an <code>async</code> loop, allowing both ROS2 callbacks and <code>asyncio</code> tasks to be processed concurrently. The <code>await asyncio.sleep(0.05)</code> ensures that the event loop can run other <code>asyncio</code> tasks.</p> </li> </ul>"},{"location":"ai/chat/running/","title":"Running with Docker","text":"<p>This project can be run using Docker and Docker Compose. Install it from here if not already available.</p> <p>IMPORTANT: Make sure to also clone the <code>ric-messages</code> git submodule located in <code>src</code> folder with:</p> <pre><code>git submodule update --init\n</code></pre>"},{"location":"ai/chat/running/#running","title":"Running","text":""},{"location":"ai/chat/running/#docker-compose-recommended","title":"Docker Compose (recommended)","text":"<p>Start the required ROS2 nodes as this node waits on them to become available: <code>ros_stt</code>, <code>ros_llm</code> and <code>ros_tts</code>.</p> <p>You can run the ROS2 chat node using:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will first build or pull the required docker images and start the chat ROS2 node that provides the <code>/chat</code> service.</p> <p>Important: Do note that the ROS2 node makes use of <code>rmw_zenoh</code> for ROS2 communication. Use the provided zenoh_router for this purpose.</p> <p>Send ROS2 messages according to the usage below.</p>"},{"location":"ai/chat/running/#manual","title":"Manual","text":"<p>Follow the relevant ROS2 documentation on how to build packages and run nodes.</p> <p>Note: The recommended way to build the ROS2 node as of writing is using <code>colcon</code>.</p>"},{"location":"ai/chat/running/#services","title":"Services","text":""},{"location":"ai/chat/running/#chat-node","title":"<code>chat-node</code>","text":"<p>This service is responsible for running the ROS2 chat node, providing the interface for the whole chat pipeline including STT, LLM and TTS.</p> <ul> <li>Uses <code>harbor.hb.dfki.de/helloric/ros_chat:latest</code> (VPN required) or builds from the local Dockerfile</li> <li>Provides a ROS2 service at <code>/chat</code> which</li> <li>Sends audio bytes to the <code>ros_stt</code> node and receives the transcribed audio as well as the language as language code (see AudioBytesToText.srv in ric-messages).</li> <li>Sends the transcribed audio from <code>ros_stt</code> to <code>ros_llm</code> and receives the response as string (see LLMChat.srv in ric-messages).</li> <li>Sends the response from <code>ros_llm</code> and the language from <code>ros_stt</code> to <code>ros_tts</code> and receives audio bytes back (see TextToAudioBytes.srv in ric-messages).</li> <li>The response consists of the language code from <code>ros_stt</code>, the response and extracted emotion from <code>ros_llm</code> and the audio bytes from <code>ros_tts</code> (see Chat.srv in ric-messages).</li> <li>It uses Zenoh as RMW implementation by default. To change it, refer to the <code>zenoh_router</code> documentation.</li> </ul>"},{"location":"ai/chat/running/#environment","title":"Environment","text":"Key Description Value <code>ROS_AUTOMATIC_DISCOVERY_RANGE</code> Disables automatic discovery in ROS2. <code>OFF</code> <code>RMW_IMPLEMENTATION</code> ROS2 middleware implementation. <code>rmw_zenoh_cpp</code> <code>ZENOH_ROUTER_CHECK_ATTEMPTS</code> Number of attempts to check for Zenoh router. <code>0</code> means wait indefinitely. <code>0</code> <code>ZENOH_CONFIG_OVERRIDE</code> Zenoh configuration override, see rmw_zenoh. <code>mode=\"client\";connect/endpoints=[\"tcp/host.docker.internal:7447\"]</code> <code>PYTHONUNBUFFERED</code> Prevents Python from buffering stdout and stderr. <code>1</code>"},{"location":"ai/chat/running/#usage","title":"Usage","text":"<p>Create a ROS2 client for the <code>/chat</code> service and call it. The service uses the <code>ric_messages/srv/Chat</code> interface. For usage examples, check out service.</p>"},{"location":"ai/chat/service/","title":"Chat Service","text":"<p>The <code>chat</code> service is a ROS2 node that orchestrates a full speech-to-speech chat pipeline. It takes an audio input, processes it through Speech-to-Text (STT), a Large Language Model (LLM), and Text-to-Speech (TTS) to generate a spoken response.</p>"},{"location":"ai/chat/service/#dependencies","title":"Dependencies","text":"<p>The <code>chat_node</code> requires the following services to be running and available:</p> <ul> <li><code>/stt</code> (<code>ric_messages/srv/AudioBytesToText</code>)</li> <li><code>/llm</code> (<code>ric_messages/srv/LLMChat</code>)</li> <li><code>/tts</code> (<code>ric_messages/srv/TextToAudioBytes</code>)</li> </ul> <p>The node will wait indefinitely for these services to become available before it starts processing requests.</p>"},{"location":"ai/chat/service/#how-to-run-the-service","title":"How to Run the Service","text":"<p>To run the chat service, you first need to source your ROS2 environment and then use the <code>ros2 run</code> command.</p> <pre><code># Source your ROS2 workspace\nsource install/setup.bash\n\n# Run the node\nros2 run chat service\n</code></pre> <p>When running the service in Docker, you can enter the container with the following command, where the above steps are already done:</p> <pre><code>docker exec -it chat-node bash\n</code></pre>"},{"location":"ai/chat/service/#parameters","title":"Parameters","text":"<p>You can customize the behavior of the node by passing the following ROS parameters.</p> <pre><code>ros2 run chat service --ros-args -p &lt;parameter&gt;:=&lt;value&gt;\n</code></pre> Parameter Description Default <code>stt_service</code> The service name for the Speech-to-Text (STT) node. <code>/stt</code> <code>llm_service</code> The service name for the Large Language Model (LLM) node. <code>/llm</code> <code>tts_service</code> The service name for the Text-to-Speech (TTS) node. <code>/tts</code>"},{"location":"ai/chat/service/#how-to-call-the-service","title":"How to Call the Service","text":"<p>You can call the <code>/chat</code> service using the <code>ros2 service call</code> command or the provided Python script.</p>"},{"location":"ai/chat/service/#service-definition","title":"Service Definition","text":""},{"location":"ai/chat/service/#chat","title":"<code>/chat</code>","text":"<p>To call the service from the command line, you need to provide the audio as an array of bytes.</p> <ul> <li>Type: <code>ric_messages/srv/Chat</code></li> <li>Description: Takes an audio input, processes it through STT, LLM, and TTS, and returns the generated audio response along with metadata.</li> <li>Request: <code>uint8[] audio</code></li> <li>Response: <code>string language</code>, <code>string text</code>, <code>string emotion</code>, <code>uint8[] audio</code></li> </ul> <pre><code>ros2 service call /chat ric_messages/srv/Chat \"{'audio': [0, 1, 2, ...]}\"\n</code></pre> <p>Note: Providing a raw audio byte array via the command line is impractical for real audio files. This method is primarily for testing with very short or empty audio clips.</p>"},{"location":"ai/chat/service/#using-the-example-python-script","title":"Using the Example Python Script","text":"<p>For actual use, it is recommended a ROS2 client like <code>call_chat_service.py</code>, which can read an audio file and send the ROS2 request.</p> <p><code>call_chat_service.py</code> can be used as test script, while the service is running. Use it as follows:</p> <pre><code>python3 call_chat_service.py &lt;input.wav&gt; &lt;output.wav&gt;\n</code></pre> <ul> <li><code>&lt;input.wav&gt;</code>: The path to the input WAV file to be sent to the service.</li> <li><code>&lt;output.wav&gt;</code>: The path where the generated output WAV file will be saved.</li> </ul> <p>Note that this will only output the response audio. Language, text and emotion will be discarded in the script.</p> <p>Important: Before running the script, you may need to configure the output audio parameters (sample rate, bit depth, channels) at the top of the <code>call_chat_service.py</code> file to match the format produced by your TTS service. The script requires <code>rclpy</code> to be able to directly call the <code>/chat</code> service. The devcontainer already contains all requirements.</p>"},{"location":"ai/chat/service/#response-example","title":"Response Example","text":"<p>A successful call will return a response object containing the generated audio and metadata:</p> <pre><code>response:\nric_messages.srv.Chat_Response(\n    language='en',\n    text='Hello! How can I help you today?',\n    emotion='calm',\n    audio=[...]\n)\n</code></pre>"},{"location":"tour/tour-manager-ui/","title":"tour-manager-ui Documentation","text":"<p>This UI runs separately from the robot within the network and is used for configuring and controlling tours that will later be executed by the robot.</p>"},{"location":"tour/tour-manager-ui/#contents-of-the-documentation","title":"Contents of the Documentation","text":"<ul> <li>Technical Overview</li> <li>API-Integration (the endpoints)</li> <li>UI Functions &amp; Logic</li> </ul>"},{"location":"tour/tour-manager-ui/backend_api/","title":"Backend API","text":""},{"location":"tour/tour-manager-ui/backend_api/#backend-integration-data-flow","title":"Backend Integration &amp; Data Flow","text":"<ul> <li>The Tour Manager UI is connected to a FastAPI backend via HTTP and WebSocket connections. This backend communicates with ROS2 services to send and receive information about tours and system states.</li> </ul>"},{"location":"tour/tour-manager-ui/backend_api/#goal-of-the-api-integration","title":"Goal of the API Integration","text":"<ul> <li>The purpose of the interface is to enable the creation, editing, and transfer of tours, as well as to transmit status information such as the robot\u2019s position or system messages to the UI in real time.</li> </ul>"},{"location":"tour/tour-manager-ui/backend_api/#communication-types","title":"Communication Types","text":"<ul> <li>HTTP (e.g. <code>fetch</code> in Svelte)</li> </ul> <p>Retrieving tour data</p> <p>Creating new tours</p> <p>Deleting or updating tours</p> <ul> <li>WebSocket Used for:</li> </ul> <p>Transmitting live data (e.g., feedback on robot status)</p> <p>State changes such as starting or stopping a tour</p>"},{"location":"tour/tour-manager-ui/backend_api/#websocket-implementation-in-the-ui","title":"WebSocket Implementation in the UI","text":"<p>The WebSocket connection is set up in the file routes/edit_tour/+page.svelte to ensure that the tour detail view remains synchronized with the backend in real time.</p> <ol> <li>open <code>ws://&lt;HOST&gt;:7000/ws</code></li> <li>on <code>open</code> -&gt; send <code>{\"action\":\"subscribe\",\"tourId\": \"&lt;current tour id&gt;\"}</code></li> <li> <p>on <code>message</code> with <code>{\"type\":\"update\",\"data\":{\u2026}}</code> -&gt; merge into local tour state</p> </li> <li> <p>Dev toggle: Disabled by default via <code>useWebSocket = false</code> to avoid errors when no WS server is running. When set to <code>true</code>, the UI will attempt the connection on mount.</p> </li> <li> <p>Test action: The button \"Send test message to WS\" calls <code>sendWsMessage(...)</code> if the socket isnt connected, the console logs a warning and nothing is sent</p> </li> <li> <p>Current status/prerequisite: The UI code is in place but wont work until a backend WebSocker endpoint exists at <code>ws://&lt;HOST&gt;:7000/ws</code> that accepts the <code>subscribe</code> message and emits <code>update</code> events. Once that server is available, set <code>useWebSocket = true</code> and adjust <code>&lt;HOST&gt;</code> (or configure a proxy) to enable live updates.</p> </li> </ol> <p>Example payloads - Subscribe : <code>{\"action\":\"subscribe\",\"tourId\":\"tour_eso\"}</code> - Update (from server): <code>{\"type\":\"update\",\"data\":{\"name\":\"ESO Tour\",\"steps\":[\u2026]}}</code></p> <ul> <li>Backend teams can implement any message schema, but the UI currently expects the above <code>subscribe</code> and <code>update</code> shapes.</li> </ul>"},{"location":"tour/tour-manager-ui/backend_api/#ui-files-related-to-the-backend","title":"UI Files Related to the Backend","text":"<p><code>lib/state/server_state.svelte</code></p> <p>Manages the global list of tours (tours)</p> <p>Updated based on API responses</p> <p><code>routes/+page.svelte</code></p> <p>Sends HTTP requests (e.g., via fetch) to API endpoints</p> <p>Creates new tours and navigates to the edit view</p> <p><code>routes/edit_tour/+page.svelte</code></p> <p>Allows editing of individual tours with direct state updates</p> <p><code>lib/state/ws.ts</code> (falls vorhanden)</p> <p>Establishes and manages the WebSocket connection</p>"},{"location":"tour/tour-manager-ui/datenstruktur/","title":"Data Structure \u2013 Tour Data &amp; Markers","text":"<p>The tour data used by the application is stored in the static/data/tours/ directory as JSON files. These files contain complete information for each individual tour, including its steps and destinations (markers). When the application loads, this data is transferred into the global state and displayed in the UI.</p>"},{"location":"tour/tour-manager-ui/datenstruktur/#tour-overview-tour_overviewjson","title":"Tour Overview \u2013 <code>tour_overview.json</code>","text":"<ul> <li>This file contains a short list of all available tours:</li> </ul> <pre><code>[\n  {\n    \"id\": \"tour_eso\",\n    \"name\": \"ESO-Tour\",\n    \"short_desc\": \"Tour for first semesters\"\n  },\n  {\n    \"id\": \"tour_school\",\n    \"name\": \"schoolltour\",\n    \"short_desc\": \"Tour for students\"\n  },\n  {\n    \"id\": \"tour_tourism\",\n    \"name\": \"Bremen Tourism\",\n    \"short_desc\": \"Tour for bremen tourism\"\n  }\n]\n</code></pre>"},{"location":"tour/tour-manager-ui/datenstruktur/#tourdetails","title":"Tourdetails","text":"<ul> <li>tour_eso.json</li> <li>This file contains all information about a tour, including its individual steps:</li> </ul> <p>```json {   \"id\": \"tour_eso\",   \"name\": \"ESO-Tour\",   \"short_desc\": \"Tour for the first semesters\",   \"steps\": [     {       \"id\": \"step_reception_intro\",       \"title\": \"greeting\",       \"expanded\": false,       \"isEditing\": false,       \"content\": \"The reception, where we welcome the first-year students\",       \"editingContent\": false,       \"destination\": {         \"marker\": \"reception_area\"       }     }   ] } ````</p>"},{"location":"tour/tour-manager-ui/datenstruktur/#marker-information","title":"Marker Information","text":"<ul> <li>The file marker.json contains a list of locations (so-called \u201cmarkers\u201d) that serve as destinations within tours. Each marker has a unique ID, a description, and a position within the space:</li> </ul> <p>```json {   \"reception_area\": {     \"description\": \"reception area on the ground floor\",     \"position\": {       \"x\": 2.3,       \"y\": 0.0,       \"z\": -1.4     }   },   \"room_101\": {     \"description\": \"seminar room 101\",     \"position\": {       \"x\": 5.1,       \"y\": 0.0,       \"z\": 3.2     }   },   \"lab_corner\": {     \"description\": \"corner in the labrotory area\",     \"position\": {       \"x\": -1.8,       \"y\": 0.0,       \"z\": 4.0     }   } } ````</p>"},{"location":"tour/tour-manager-ui/datenstruktur/#linking-with-tour-data","title":"Linking with Tour Data","text":"<p>In the JSON files of individual tours (e.g., tour_eso.json), each step references a marker via the attribute destination.marker. This marker ID must match an entry in marker.json for the step to be correctly assigned.</p>"},{"location":"tour/tour-manager-ui/datenstruktur/#note-on-expandability","title":"Note on Expandability","text":"<p>The data structure is easy to extend: New tours can be added simply by creating additional JSON files in the static/data/tours/ directory. Markers can be centrally managed in marker.json and reused across any number of tours.</p>"},{"location":"tour/tour-manager-ui/interface/","title":"Navigation &amp; Use of the UI","text":"<p>To start the user interface locally, run the following commands in the terminal:</p> <p>npm install , npm run smui npm run dev you also need to install OpenVPN Connect and log in with your dfki gitlab account.</p> <p>The application will then be accessible at: : http://localhost:5173/</p>"},{"location":"tour/tour-manager-ui/interface/#functions","title":"Functions","text":"<p>The start view at <code>http://localhost:5173/</code> displays an overview of all existing tours:</p> <p></p> <ul> <li>Tour Overview \u2013 Shows all existing tours with title and description</li> <li>Start Tour \u2013 Opens the detail view</li> <li>Create Tour \u2013 Creates a new tour with a generated ID and an empty step list</li> <li>Edit Tour \u2192 Add steps</li> <li>Reorder Steps (via drag &amp; drop)</li> <li>Editable Titles &amp; Descriptions \u2013 Can be changed directly in place</li> <li>Editable Step Content \u2013 Each step\u2019s content can be modified in a text field</li> </ul> <p>Tour images are loaded dynamically from the path: (<code>/images/${tour.id}.jpg</code>)</p>"},{"location":"tour/tour-manager-ui/interface/#interactions","title":"Interactions","text":"<ul> <li>Click on a card \u2192 Edit the selected tour </li> <li>Click on the \u201c+\u201d icon \u2192 Creates a new tour and immediately navigates to <code>/edit_tour/&lt;id&gt;</code> </li> <li>Click on \u201c+ Add Step\u201d \u2192 Inserts a new step into the tour</li> <li>Fields are directly editable</li> <li>Drag &amp; drop sorting is implemented using <code>runic-reorder</code></li> </ul>"},{"location":"tour/tour-manager-ui/interface/#ui-components-smui","title":"UI-Components (SMUI)","text":"<ul> <li><code>Card</code>, <code>Media</code>, <code>IconButton</code>, <code>List</code>, <code>Accordion</code></li> <li>Icons: <code>play_arrow</code>, <code>pause</code>, <code>delete</code>, <code>add</code>, <code>drag_indicator</code></li> <li>Layout: <code>LayoutGrid</code> and <code>MainNav</code></li> </ul>"},{"location":"tour/tour-manager-ui/interface/#logic-state-management","title":"Logic &amp; State Management","text":"<p>The core logic of the UI is based on a global state, managed in the file <code>server_state.svelt</code>.  Here, a reactive <code>tour</code> list is defined, containing all tours. This list serves as the source for rendering the start page (<code>+page.svelt</code>) and is automatically updated whenever content changes.</p> <p>When the application loads, JSON files from the directory <code>static/data/tours</code> (e.g.,<code>tour_school.json</code>,<code>tour_tourism.json</code>) are read in. Each file contains a complete tour including ID, title, description, and steps.</p> <p>When a tour is edited or created, updates are applied directly to the global state\u2014meaning changes to the title, description, or steps are instantly reflected in the UI.</p> <p>A tour ID is automatically generated when creating a new tour (<code>timestamp-base</code>) and used to navigate to the detail view (<code>/edit_tour/&lt;id&gt;</code>).</p> <p>Tour steps are stored as a list of objects and can be reordered via drag &amp; drop (<code>runic-reorder</code>) or dynamically extended. Changes remain in sync between the tour list and the displayed view.</p>"},{"location":"tour/tour-manager-ui/interface/#key-files","title":"Key Files","text":"<p><code>+page.svelte</code> \u2192 Main overview page, located in routes/+page.svelte</p> <p><code>routes/edit_tour/+page.svelte</code> \u2192 Tour editing view, accessed by clicking on a tour in the overview</p> <p><code>server_state.svelte</code> \u2192 Global state file containing the <code>tours</code> list; tour data is loaded from JSON files in static/data/tours/</p>"},{"location":"tour/tour-manager-ui/interface/#note","title":"Note","text":"<p>This interface is not installed directly on the robot; it runs separately within the network. All changes to tours are stored dynamically in the global state (server_state).</p>"},{"location":"tour/tour-persistence/","title":"Tour Persistence Node","text":""},{"location":"tour/tour-persistence/#overview","title":"Overview","text":"<p>This package provides a ROS node for managing persistent tour-related data in MongoDB.</p> <p>It supports operations like creating, updating, reading and deleting tours.</p> <p>The core implementation is located in <code>src/tour_persistence/tour_persistence/node.py</code>.</p>"},{"location":"tour/tour-persistence/#workflow","title":"Workflow","text":"<p>The following diagram illustrates the interaction between the client, the ROS node, and the MongoDB backend during a typical operation:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server as ROS Node (node.py)\n    participant Converter as Converter (ros_to_mongodb.py)\n    participant DBClient as DBClient(mongodb_server/base_motor.py)\n    participant MongoDB\n\n    Client-&gt;&gt;Server: Call service '/tour_create'(send TourCreate request with Tour)\n    activate Server\n    note right of Server: on_create starts\n\n    %% Insert MongoDB processing here\n    Server-&gt;&gt;Converter: Transform ROS msg to MongoDB-friendly format\n    Converter--&gt;&gt;Server: Return transformed data\n    Server-&gt;&gt;DBClient: Connect to MongoDB via Database client\n    DBClient-&gt;&gt;MongoDB: Insert transformed data\n    MongoDB--&gt;&gt;DBClient: Acknowledge insertion\n\n    Server--&gt;&gt;Client: Return TourCreate response (success: true/false)\n    deactivate Server\n    note right of Server: on_create ends\n</code></pre>"},{"location":"tour/tour-persistence/#services","title":"Services","text":"<p>The node provides the following services:</p> <ul> <li> <p><code>/tour_create</code>: Create a new tour in the database.</p> </li> <li> <p><code>/tour_update</code>: Update an existing tour in the database.</p> </li> <li> <p><code>/tour_delete</code>: Delete a tour from the database.</p> </li> <li> <p><code>/tour_list</code>: List names of tours in the database.</p> </li> <li> <p><code>/tour_get</code>: Get a specific tour by name from the database.</p> </li> </ul> <p>Format of the service requests and responses is defined in the ric_messages package.</p>"},{"location":"tour/tour-persistence/#testing","title":"Testing","text":"<p>To test the system: 1. Ensure the MongoDB container is running from the <code>mongodb_server</code> 2. In the project root, run</p> <p><pre><code>docker compose up --build\n</code></pre> 3. Then you can try to send a request using helloric_ui_com. Or you can use the command line to test the service directly:</p> <pre><code># Step 1: Enter the container\ndocker exec -it tour_persistence /bin/bash\n\n# Step 2: Then inside the container\nros2 service call /tour_list ric_messages/srv/TourList\n</code></pre> <p>If everything is working, you should see something like this in the terminal, where you ran the <code>docker compose up</code> command:</p> <p><pre><code>tour_persistence  | [INFO] [1753977953.019991590] [persistent_tour_node]: Got tour list request\ntour_persistence  | [INFO] [1753977953.022035559] [persistent_tour_node]: Found 8 tours\n</code></pre> And in terminal where you ran the <code>ros2 service call</code> command, you should see the response:</p> <pre><code>requester: making request: ric_messages.srv.TourList_Request()\n\nresponse:\nric_messages.srv.TourList_Response(name=['sample_tour2', 'sample_tour1', 'davaj', 'davaj_paka', 'ESO-Tour', 'tour_eso', 'tour_school', 'tour_tourism'])\n</code></pre> <p>You can also add a dummy tour to the database using the following command:</p> <p><pre><code>ros2 service call /tour_create ric_messages/srv/TourCreate \"{\n  tour: {\n    tour_name: 'test_tour',\n    map_name: 'test_map',\n    step: [\n      {\n        description: 'Go to entrance',\n        destination: 'entrance',\n        dialogue: 'Welcome to the tour!'\n      },\n      {\n        description: 'Show main hall',\n        destination: 'main_hall',\n        dialogue: 'This is the main hall.'\n      }\n    ]\n  }\n}\"\n</code></pre> And your response should look like this:</p> <pre><code>requester: making request: ric_messages.srv.TourCreate_Request(tour=ric_messages.msg.Tour(tour_name='test_tour', map_name='test_map', step=[ric_messages.msg.Step(index=0, description='Go to entrance', destination='entrance', dialogue='Welcome to the tour!'), ric_messages.msg.Step(index=0, description='Show main hall', destination='main_hall', dialogue='This is the main hall.')]))\n\nresponse:\nric_messages.srv.TourCreate_Response(success=True)\n</code></pre>"},{"location":"tour/mapdesc-persistence/","title":"Mapdesc Persistence Node","text":""},{"location":"tour/mapdesc-persistence/#overview","title":"Overview","text":"<p>This package provides a ROS node for managing persistent map-related data in MongoDB.</p> <p>It supports operations like creating, updating, reading and deleting maps. Also it allows editing maps' walls, areas and markers.</p> <p>The core implementation is located in <code>src/mapdesc_persistence/mapdesc_persistence/node.py</code>.</p>"},{"location":"tour/mapdesc-persistence/#workflow","title":"Workflow","text":"<p>The following diagram illustrates the interaction between the client, the ROS node, and the MongoDB backend during a typical operation:</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server as ROS Node (node.py)\n    participant Converter as Converter (ros_to_mongodb.py)\n    participant DBClient as DBClient(mongodb_server/base_motor.py)\n    participant MongoDB\n\n    Client-&gt;&gt;Server: Call service '/mapdesc/create'(send mapdesc_msgs/srv/MapCreate request with Map)\n    activate Server\n    note right of Server: on_create starts\n\n    %% Insert MongoDB processing here\n    Server-&gt;&gt;Converter: Transform ROS msg to MongoDB-friendly format\n    Converter--&gt;&gt;Server: Return transformed data\n    Server-&gt;&gt;DBClient: Connect to MongoDB via Database client\n    DBClient-&gt;&gt;MongoDB: Insert transformed data\n    MongoDB--&gt;&gt;DBClient: Acknowledge insertion\n\n    Server--&gt;&gt;Client: Return MapCreate response (success: true/false)\n    deactivate Server\n    note right of Server: on_create ends\n</code></pre>"},{"location":"tour/mapdesc-persistence/#services","title":"Services","text":"<p>The node provides the following services For managing maps, you can use the following services:</p> <ul> <li> <p><code>/mapdesc/create</code>: Create a new map.</p> </li> <li> <p><code>/mapdesc/update</code>: Update an existing map.</p> </li> <li> <p><code>/mapdesc/read</code>: Read a map by its name.</p> </li> <li> <p><code>/mapdesc/delete</code>: Delete a map by its name.</p> </li> <li> <p><code>/mapdesc/list</code>: List all maps.</p> </li> </ul> <p>And for managing map elements, you can use the following services (you should change <code>&lt;attribute name&gt;</code> to the actual attribute you want to edit like <code>wall</code>, <code>area</code> or <code>marker</code>):</p> <ul> <li> <p><code>/mapdesc/&lt;attribute name&gt;/create</code>: Create a new attribute in the map.</p> </li> <li> <p><code>/mapdesc/&lt;attribute name&gt;/update</code>: Update an existing attribute in the map.</p> </li> <li> <p><code>/mapdesc/&lt;attribute name&gt;/delete</code>: Delete an attribute by its ID.</p> </li> <li> <p><code>/mapdesc/&lt;attribute name&gt;/list</code>: List all elements in the specified attribute.</p> </li> </ul> <p>Format of the service requests and responses is defined in the mapdesc_msgs package.</p>"},{"location":"tour/mapdesc-persistence/#testing","title":"Testing","text":"<p>To test the system: 1. Ensure the MongoDB container is running from the <code>mongodb_server</code></p> <ol> <li>In the project root, run</li> </ol> <pre><code>docker compose up --build\n</code></pre> <ol> <li>Then you can try to send a request. You can use the new terminal for this purpose: <pre><code># Step 1: Enter the container\ndocker exec -it mapdesc_persistence /bin/bash\n\n# Step 2: Then inside the container\nros2 service call /mapdesc/list mapdesc_msgs/srv/MapList\n</code></pre></li> </ol> <p>If everything is working, you should see something like this in the terminal (if you have no maps yet):</p> <pre><code>requester: making request: mapdesc_msgs.srv.MapList_Request()\n\nresponse:\nmapdesc_msgs.srv.MapList_Response(map=[])\n</code></pre> <p>You can also add some dummy map <pre><code>ros2 service call /mapdesc/create mapdesc_msgs/srv/MapCreate \"{\n  map: {\n    name: 'test_map',\n    description: 'Dummy map for testing',\n    size: {width: 10.0, height: 5.0, length: 3.0},\n    resolution: 0.05,\n    origin: {x: 0.0, y: 0.0, z: 0.0},\n    marker: [],\n    area: [],\n    wall: [],\n    path: [],\n    ext: [],\n    lane_graph: {\n      nodes: [],\n      edges: []\n    }\n  }\n}\"\n</code></pre></p> <p>Then if you call service <code>/mapdesc/list</code> again, you should see the map in the response:</p> <pre><code>requester: making request: mapdesc_msgs.srv.MapList_Request()\n\nresponse:\nmapdesc_msgs.srv.MapList_Response(map=[mapdesc_msgs.msg.Map(name='test_map', description='Dummy map for testing', size=mapdesc_msgs.msg.Dimension(width=0.0, height=0.0, length=0.0), resolution=0.05, origin=geometry_msgs.msg.Vector3(x=0.0, y=0.0, z=0.0), marker=[], area=[], wall=[], path=[], ext=[], lane_graph=mapdesc_msgs.msg.LaneGraph(nodes=[], edges=[]))])\n</code></pre>"},{"location":"tour/mongodb/","title":"MongoDB Server","text":"<p>This repository contains two main usages:</p> <ul> <li>Running a standalone MongoDB server via Docker.</li> <li>A Python package with a database client, which is used in projects like tour_persistence_node.</li> </ul>"},{"location":"tour/mongodb/mongodb_server/","title":"mongodb_server","text":""},{"location":"tour/mongodb/mongodb_server/#structure","title":"Structure","text":"<p>The <code>helloric</code> database is MongoDB database containing collections <code>maps</code> and <code>tours</code>. </p>"},{"location":"tour/mongodb/mongodb_server/#maps","title":"Maps","text":"<p>Each document in this collection represents an individual map (primary key is <code>name</code>) with the following structure:</p> <pre><code>erDiagram\n\n    MAP {\n        string description\n        string name\n        int[3] origin\n        float resolution\n    }\n\n    WALL {\n        string name\n        string type\n    }\n\n    AREA {\n        string name\n        string type\n        string area_type\n        int[3] color\n    }\n\n    MARKER {\n        string name\n        float radius\n        int[3] color\n    }\n\n    DATA {\n        float[3] size\n    }\n\n    POSE {\n        float[3] position\n        float[4] orientation\n    }\n\n    MAP ||--o{ WALL : contains_list_of\n    MAP ||--o{ AREA : contains_list_of\n    MAP ||--o{ MARKER : contains_list_of\n\n    WALL ||--|| DATA : has\n    AREA ||--|| DATA : has\n    DATA ||--|| POSE : has\n    MARKER ||--|| POSE : has\n</code></pre>"},{"location":"tour/mongodb/mongodb_server/#tours","title":"Tours","text":"<p>Each document in this collection represents an individual tour (primary key is <code>tour_name</code>) with the following structure:</p> <pre><code>erDiagram\n    TOUR {\n        string tour_name\n        string map_name\n    }\n\n    STEP {\n        uint16 index\n        string desctribtion\n        string destination\n        string dialogue\n    }\n\n    TOUR ||--o{ STEP : contains_list_of</code></pre>"},{"location":"tour/mongodb/mongodb_server/#start-and-test","title":"Start and test","text":"<p>To start the MongoDB server, simply open this repository in a Docker container (e.g., using VS Code).</p> <p>Then go into the <code>mongodb_server</code> directory: <pre><code>cd mongodb_server\n</code></pre></p> <p>In <code>test.py</code>, you\u2019ll find two helper methods: - One to view the current documents in the database. - Another to clear (drop) the database.</p> <p>To use them:</p> <ol> <li> <p>Open the <code>main()</code> function in <code>test.py</code>.</p> </li> <li> <p>Uncomment the method you want to run.</p> </li> <li> <p>Execute the script inside the Docker container:</p> </li> </ol> <pre><code>python3 test.py\n</code></pre>"},{"location":"tour/mongodb/mongodb_server/#load-maps-inside-the-database","title":"Load maps inside the database","text":"<p>You can load maps using mongodb server as package and running 'mapdesc_persistence' node.</p> <p>But also you can use the <code>load_maps.py</code> script in this repository to load maps into the database from yaml files:</p> <ol> <li> <p>Store you yaml files in the <code>maps</code> folder</p> </li> <li> <p>In file <code>load_maps.py</code> change name of the yaml file (code line 117)</p> </li> <li> <p>Run the script inside the container:</p> </li> </ol> <pre><code>python3 load_maps.py\n</code></pre>"},{"location":"tour/mongodb/python_package/","title":"python_package","text":"<p>MongoDB Server as a Python package.</p>"},{"location":"tour/mongodb/python_package/#usage","title":"Usage","text":"<ol> <li>Clone the repository.</li> <li>Install the package using pip: <pre><code>pip install .\n</code></pre>  Now you can connect to database like this: <pre><code>    from mongodb_server.base_motor import Database\n\n    client = Database()\n    client.connect()\n    db = Database.get_database()\n</code></pre></li> </ol>"},{"location":"helloric-ui-com/","title":"HelloRIC UI Communication","text":"<p>A ROS2 Jazzy FastAPI server that acts as a bridge between web-based user interfaces and the ROS2 ecosystem for the HelloRIC robot platform.</p> <p>The Tour Guide Manager App (TGM) and the Emotion interface communicate with the ROS2 backend using this communication layer:</p> <ul> <li>TGM consumes RESTful HTTP endpoints for tour management operations</li> <li>Emotion interface connects via WebSocket at the <code>/ws</code> endpoint for real-time audio and emotion data</li> <li>The FastAPI server provides bidirectional communication between web clients and ROS2 services</li> </ul> <p>The system supports real-time audio processing, emotion display, microphone control, and comprehensive tour management through a unified web interface.</p> <p>The FastAPI server is published using Uvicorn.</p>"},{"location":"helloric-ui-com/#architecture","title":"Architecture","text":"<p>The following graph shows all current relations between the interacting ROS2-Nodes.</p> <p>It was generated using ROS2-Graph.</p> <p>To produce another diagram, follow the install instructions and start all relevant nodes.</p> <pre><code>flowchart LR\n\n/HelloRIC_ui_com[ /HelloRIC_ui_com ]:::main\n/llm_node[ /llm_node ]:::main\n/stt_node[ /stt_node ]:::main\n/tts_node[ /tts_node ]:::main\n/chat_node[ /chat_node ]:::main\n/chat_node[ /chat_node ]:::node\n/play_audio([ /play_audio&lt;br&gt;ric_messages/msg/PlayAudio ]):::bugged\n/microphone/set_enabled([ /microphone/set_enabled&lt;br&gt;std_srvs/srv/SetBool ]):::bugged\n/HelloRIC_ui_com/get_type_description[/ /HelloRIC_ui_com/get_type_description&lt;br&gt;type_description_interfaces/srv/GetTypeDescription \\]:::bugged\n/tour_create[/ /tour_create&lt;br&gt;ric_messages/srv/TourCreate \\]:::bugged\n/tour_delete[/ /tour_delete&lt;br&gt;ric_messages/srv/TourDelete \\]:::bugged\n/tour_get[/ /tour_get&lt;br&gt;ric_messages/srv/TourGet \\]:::bugged\n/tour_list[/ /tour_list&lt;br&gt;ric_messages/srv/TourList \\]:::bugged\n/tour_update[/ /tour_update&lt;br&gt;ric_messages/srv/TourUpdate \\]:::bugged\n/clear_history[/ /clear_history&lt;br&gt;std_srvs/srv/Empty \\]:::bugged\n/llm[/ /llm&lt;br&gt;ric_messages/srv/LLMChat \\]:::service\n/llm_node/get_type_description[/ /llm_node/get_type_description&lt;br&gt;type_description_interfaces/srv/GetTypeDescription \\]:::bugged\n/stt[/ /stt&lt;br&gt;ric_messages/srv/AudioBytesToText \\]:::service\n/stt_node/get_type_description[/ /stt_node/get_type_description&lt;br&gt;type_description_interfaces/srv/GetTypeDescription \\]:::bugged\n/tts[/ /tts&lt;br&gt;ric_messages/srv/TextToAudioBytes \\]:::service\n/tts_node/get_type_description[/ /tts_node/get_type_description&lt;br&gt;type_description_interfaces/srv/GetTypeDescription \\]:::bugged\n/chat[/ /chat&lt;br&gt;ric_messages/srv/Chat \\]:::bugged\n/chat_node/get_type_description[/ /chat_node/get_type_description&lt;br&gt;type_description_interfaces/srv/GetTypeDescription \\]:::bugged\n\n/HelloRIC_ui_com --&gt; /play_audio\n/HelloRIC_ui_com --&gt; /microphone/set_enabled\n/HelloRIC_ui_com/get_type_description o-.-o /HelloRIC_ui_com\n/clear_history o-.-o /llm_node\n/llm o-.-o /llm_node\n/llm_node/get_type_description o-.-o /llm_node\n/stt o-.-o /stt_node\n/stt_node/get_type_description o-.-o /stt_node\n/tts o-.-o /tts_node\n/tts_node/get_type_description o-.-o /tts_node\n/chat o-.-o /chat_node\n/chat_node/get_type_description o-.-o /chat_node\n/HelloRIC_ui_com &lt;-.-&gt; /tour_create\n/HelloRIC_ui_com &lt;-.-&gt; /tour_delete\n/HelloRIC_ui_com &lt;-.-&gt; /tour_get\n/HelloRIC_ui_com &lt;-.-&gt; /tour_list\n/HelloRIC_ui_com &lt;-.-&gt; /tour_update\n/chat_node &lt;-.-&gt; /llm\n/chat_node &lt;-.-&gt; /stt\n/chat_node &lt;-.-&gt; /tts\n/chat_node &lt;-.-&gt; /llm\n/chat_node &lt;-.-&gt; /stt\n/chat_node &lt;-.-&gt; /tts\n\n\nsubgraph keys[&lt;b&gt;Keys&lt;b/&gt;]\nsubgraph nodes[&lt;b&gt;&lt;b/&gt;]\ntopicb((No connected)):::bugged\nmain_node[main]:::main\nend\nsubgraph connection[&lt;b&gt;&lt;b/&gt;]\nnode1[node1]:::node\nnode2[node2]:::node\nnode1 o-.-o|to server| service[/Service&lt;br&gt;service/Type\\]:::service\nservice &lt;-.-&gt;|to client| node2\nnode1 --&gt;|publish| topic([Topic&lt;br&gt;topic/Type]):::topic\ntopic --&gt;|subscribe| node2\nnode1 o==o|to server| action{{/Action&lt;br&gt;action/Type/}}:::action\naction &lt;==&gt;|to client| node2\nend\nend\nclassDef node opacity:0.9,fill:#2A0,stroke:#391,stroke-width:4px,color:#fff\nclassDef action opacity:0.9,fill:#66A,stroke:#225,stroke-width:2px,color:#fff\nclassDef service opacity:0.9,fill:#3B8062,stroke:#3B6062,stroke-width:2px,color:#fff\nclassDef topic opacity:0.9,fill:#852,stroke:#CCC,stroke-width:2px,color:#fff\nclassDef main opacity:0.9,fill:#059,stroke:#09F,stroke-width:4px,color:#fff\nclassDef bugged opacity:0.9,fill:#933,stroke:#800,stroke-width:2px,color:#fff\nstyle keys opacity:0.15,fill:#FFF\nstyle nodes opacity:0.15,fill:#FFF\nstyle connection opacity:0.15,fill:#FFF\n</code></pre>"},{"location":"helloric-ui-com/#requirements","title":"Requirements","text":"<ul> <li>Docker</li> <li>Visual Studio Code</li> <li>Dev Container Extension</li> <li>ROS2 Jazzy (if running outside Docker)</li> <li>Zenoh Router for ROS2 communication</li> </ul>"},{"location":"helloric-ui-com/#contributing","title":"Contributing","text":"<p>We welcome contributions to this project! Please see the contributing guidelines at <code>contributing.md</code> in the root of this repository for more information on how to get started.</p>"},{"location":"helloric-ui-com/api_endpoints/","title":"API Endpoints for Tour Guide Manager","text":"<p>When an endpoint is called, the corresponding ROS2 service is also called.</p>"},{"location":"helloric-ui-com/api_endpoints/#endpoints","title":"Endpoints","text":"<ul> <li>GET /tour : returns all the tour names.</li> <li>GET /tour/{tour_name} : returns the tour object with the tour_name. If no such tour is found, an 404 error will be thrown.</li> <li>POST /tour/ : creates a new tour using the <code>tour</code> in the request body. Will fail if a tour with the same name already exists!</li> <li>PUT /tour/ : updates an existing tour using the <code>tour</code> in the request body. Will fail if no tour exists with the <code>tour.tour_name</code>.</li> <li>DELETE /tour/{tour_name} : deletes a tour with the tour_name. Fails if no tour exists with <code>tour_name</code>.</li> </ul>"},{"location":"helloric-ui-com/api_endpoints/#but-i-am-a-visual-learner","title":"But I am a visual learner","text":"<pre><code>sequenceDiagram\n  participant ui as Tour Guide Manager UI App\n  participant http as HTTP Endpoints on helloric_ui_com\n  participant ros as ROS\n\n  ui -&gt;&gt; ui: User opens app - request all tour names\n  ui -&gt;&gt; http: GET /tour\n  http -&gt;&gt; ros: /tour_list: Empty - Service list_tour\n  ros -&gt;&gt; ros: Queries all tour names from DB\n  http -&gt;&gt; ui: RESPONSE tour_list: List[]\n\n  ui -&gt;&gt; ui: User clicks on a tour - Request a tour\n  ui -&gt;&gt; http: GET /tour/{name}\n  http -&gt;&gt; ros: /tour_get: String - Service get_tour\n  ros -&gt;&gt; ros: Queries Tour from DB with names\n  http -&gt;&gt; ui: RESPONSE tour: TourModel\n\n  ui -&gt;&gt; ui: User creates new tour\n  ui -&gt;&gt; http: POST /tour BODY TourModel\n  http -&gt;&gt; ros: /tour_create: Tour - Service create_tour\n  ros -&gt;&gt; ros: Inserts new tour in DB\n  http -&gt;&gt; ui: RESPONSE success: Bool\n\n  ui -&gt;&gt; ui: User updates an existing tour\n  ui -&gt;&gt; http: PUT /tour/{name}\n  http -&gt;&gt; ros: /tour_update: Tour - Service update_tour\n  ros -&gt;&gt; ros: Updates existing tour in DB\n  http -&gt;&gt; ui: RESPONSE success: Bool\n\n  ui -&gt;&gt; ui: User deletes a Tour\n  ui -&gt;&gt; http: DELETE /tour/{name}\n  http -&gt;&gt; ros: /tour_delete: String - Service delete_tour\n  ros -&gt;&gt; ros: Deletes tour in DB\n  http -&gt;&gt; ui: RESPONSE success: Bool</code></pre>"},{"location":"helloric-ui-com/code/","title":"Code Documentation","text":"<p>This document provides an overview of the code of the <code>helloric_ui_com</code> package.</p>"},{"location":"helloric-ui-com/code/#helloricmgr","title":"<code>HelloRICMgr</code>","text":"<p>The <code>HelloRICMgr</code> class is a ROS2-enabled WebSocket connection manager that acts as a bridge between the ROS2 ecosystem and web-based user interfaces. It extends the <code>WebSocketConnectionManager</code> to provide real-time communication capabilities between ROS2 nodes and WebSocket clients for the HelloRIC robot interface.</p> <p>Note: The manager leverages FastAPI's WebSocket support and integrates with ROS2 services to provide bidirectional communication for audio processing, tour management, and emotion display.</p>"},{"location":"helloric-ui-com/code/#attributes","title":"Attributes","text":"<p>The manager maintains the following key attributes:</p> Attribute Type Description <code>ros_node</code> ROSCom The ROS2 communication node for service calls <code>tour_name</code> List[str] List of available tour names <code>tour</code> Optional[Dict[str, object]] Current tour data <code>success</code> bool Status flag for operation success"},{"location":"helloric-ui-com/code/#main-loop","title":"Main Loop","text":"<p>The manager runs a continuous main loop that processes ROS2 messages:</p> <pre><code>async def main_loop(self):\n    while rclpy.ok():\n        if self.ros_node:\n            rclpy.spin_once(self.ros_node, timeout_sec=0.1)\n        await asyncio.sleep(0.01)\n</code></pre> <p>This loop ensures that ROS2 callbacks are processed while maintaining WebSocket responsiveness.</p>"},{"location":"helloric-ui-com/code/#client-management","title":"Client Management","text":""},{"location":"helloric-ui-com/code/#client_defaultsuser_id-str","title":"<code>client_defaults(user_id: str)</code>","text":"<ul> <li>Description: Sends initial default state to a newly connected client.</li> <li>Parameters: </li> <li><code>user_id</code> (str): Unique identifier for the WebSocket client</li> <li>Default State: </li> <li><code>microphoneActive</code>: True</li> <li><code>emotion</code>: Emotion.CALM.value</li> </ul>"},{"location":"helloric-ui-com/code/#set_microphone_stateactive-bool-user_id-str-none-none","title":"<code>set_microphone_state(active: bool, user_id: str | None = None)</code>","text":"<ul> <li>Description: Updates the microphone state for clients.</li> <li>Parameters:</li> <li><code>active</code> (bool): Whether the microphone should be active</li> <li><code>user_id</code> (str | None): Specific client ID, or None to broadcast to all clients</li> <li>Behavior: Sends JSON message with <code>microphoneActive</code> field</li> </ul>"},{"location":"helloric-ui-com/code/#audio-message-handling","title":"Audio Message Handling","text":""},{"location":"helloric-ui-com/code/#send_audio_msgaudio_bytes-text-language-emotion-namenone","title":"<code>send_audio_msg(audio_bytes, text, language, emotion, name=None)</code>","text":"<ul> <li>Description: Sends audio messages to the frontend with synchronized metadata and audio data.</li> <li>Parameters:</li> <li><code>audio_bytes</code> (array[int]): Raw audio data</li> <li><code>text</code> (str): Text content of the audio message</li> <li><code>language</code> (str): Language identifier</li> <li><code>emotion</code> (str): Emotion identifier for the message</li> <li><code>name</code> (str | None): Target client name, or None to broadcast</li> <li>Protocol: </li> <li>Generates unique message ID based on timestamp</li> <li>Sends JSON metadata with emotion, text, and message ID</li> <li>Sends binary audio data with 4-byte message ID header</li> <li>Message Format: </li> <li>JSON: <code>{\"emotion\": str, \"text\": str, \"messageId\": int}</code></li> <li>Binary: <code>[4-byte message ID][audio data]</code></li> </ul>"},{"location":"helloric-ui-com/code/#how-it-works","title":"How it Works","text":"<ol> <li> <p>Initialization: The manager initializes the WebSocket connection manager and sets up ROS2 integration.</p> </li> <li> <p>Client Connection: When a client connects via WebSocket, a unique user ID is generated and default state is sent.</p> </li> <li> <p>Audio Processing Pipeline: </p> </li> <li>Client sends audio data via WebSocket</li> <li>Manager forwards audio to ROS2 <code>/chat</code> service</li> <li>Receives processed audio response with text, language, and emotion</li> <li> <p>Sends synchronized audio and metadata back to client</p> </li> <li> <p>Service Commands: Clients can send JSON commands (e.g., \"clear_history\") that are forwarded to appropriate ROS2 services.</p> </li> <li> <p>Real-time Communication: The main loop ensures continuous processing of ROS2 messages while maintaining WebSocket connections.</p> </li> </ol>"},{"location":"helloric-ui-com/code/#tour-data-models","title":"Tour Data Models","text":""},{"location":"helloric-ui-com/code/#tourstepmodel","title":"<code>TourStepModel</code>","text":"<ul> <li>Description: Pydantic model representing a single step in a tour.</li> <li>Fields:</li> <li><code>description</code> (str): Human-readable description of the step</li> <li><code>destination</code> (Optional[str]): Target location for the step</li> <li><code>dialogue</code> (Optional[str]): Speech content for the step</li> </ul>"},{"location":"helloric-ui-com/code/#tourmodel","title":"<code>TourModel</code>","text":"<ul> <li>Description: Pydantic model representing a complete tour.</li> <li>Fields:</li> <li><code>tour_name</code> (str): Unique identifier for the tour</li> <li><code>map_name</code> (str): Associated map identifier</li> <li><code>steps</code> (List[TourStepModel]): Ordered list of tour steps</li> </ul>"},{"location":"helloric-ui-com/code/#tour-conversion-functions","title":"Tour Conversion Functions","text":""},{"location":"helloric-ui-com/code/#convert_tour_to_rostour-tourmodel-tour","title":"<code>convert_tour_to_ros(tour: TourModel) -&gt; Tour</code>","text":"<ul> <li>Description: Converts a Pydantic tour model to a ROS2 Tour message.</li> <li>Parameters: </li> <li><code>tour</code> (TourModel): Input tour model</li> <li>Returns: <code>Tour</code> - ROS2 message format</li> <li>Process: Maps Pydantic fields to ROS2 message fields and creates Step messages for each tour step</li> </ul>"},{"location":"helloric-ui-com/code/#convert_ros_to_tourros_tour-tour-dict","title":"<code>convert_ros_to_tour(ros_tour: Tour) -&gt; Dict</code>","text":"<ul> <li>Description: Converts a ROS2 Tour message to a dictionary format.</li> <li>Parameters:</li> <li><code>ros_tour</code> (Tour): Input ROS2 tour message</li> <li>Returns: <code>Dict</code> - Dictionary representation suitable for JSON serialization</li> <li>Process: Extracts tour metadata and converts step messages to dictionary format</li> </ul>"},{"location":"helloric-ui-com/code/#websocket-initialization-init_websocket","title":"WebSocket Initialization (<code>init_websocket</code>)","text":"<p>The <code>init_websocket</code> function sets up the WebSocket endpoint and message processing pipeline.</p>"},{"location":"helloric-ui-com/code/#websocket-endpoint-ws","title":"WebSocket Endpoint (<code>/ws</code>)","text":"<ul> <li>Description: Main WebSocket endpoint for real-time communication with clients.</li> <li>Features:</li> <li>Automatic client registration and cleanup</li> <li>Task management for concurrent audio processing</li> <li>Error handling and disconnection management</li> <li>Support for both binary (audio) and text (JSON) messages</li> </ul>"},{"location":"helloric-ui-com/code/#message-processing","title":"Message Processing","text":""},{"location":"helloric-ui-com/code/#process_audioaudio_bytes-user_id","title":"<code>process_audio(audio_bytes, user_id)</code>","text":"<ul> <li>Description: Processes incoming audio data from clients.</li> <li>Parameters:</li> <li><code>audio_bytes</code> (array[int]): Raw audio data from client</li> <li><code>user_id</code> (str): Client identifier</li> <li>Process:</li> <li>Disables client microphone during processing</li> <li>Creates <code>/chat</code> service request with audio data</li> <li>Calls ROS2 chat service asynchronously</li> <li>Sends processed audio response back to client</li> <li>Re-enables client microphone</li> <li>Error Handling: Resets client to default state on errors</li> </ul>"},{"location":"helloric-ui-com/code/#process_jsondata-user_id","title":"<code>process_json(data, user_id)</code>","text":"<ul> <li>Description: Processes JSON commands from clients.</li> <li>Parameters:</li> <li><code>data</code> (dict): JSON command data</li> <li><code>user_id</code> (str): Client identifier</li> <li>Supported Commands:</li> <li><code>clear_history</code>: Calls ROS2 <code>/clear_history</code> service</li> </ul>"},{"location":"helloric-ui-com/code/#http-endpoints-add_http_endpoints","title":"HTTP Endpoints (<code>add_http_endpoints</code>)","text":"<p>The system provides RESTful HTTP endpoints for tour management.</p>"},{"location":"helloric-ui-com/code/#tour-get","title":"<code>/tour</code> (GET)","text":"<ul> <li>Description: Retrieves list of available tours.</li> <li>Response: <code>List[str]</code> - Array of tour names</li> <li>ROS2 Service: <code>/tour_list</code></li> </ul>"},{"location":"helloric-ui-com/code/#tourtour_name-get","title":"<code>/tour/{tour_name}</code> (GET)","text":"<ul> <li>Description: Retrieves specific tour details.</li> <li>Parameters: <code>tour_name</code> (str) - Name of the tour to retrieve</li> <li>Response: <code>Dict</code> - Tour data with steps and metadata</li> <li>ROS2 Service: <code>/tour_get</code></li> <li>Error Handling: Returns 404 if tour not found</li> </ul>"},{"location":"helloric-ui-com/code/#tour-post","title":"<code>/tour</code> (POST)","text":"<ul> <li>Description: Creates a new tour.</li> <li>Request Body: <code>TourModel</code> - Complete tour definition</li> <li>Response: <code>{\"success\": bool}</code> - Creation status</li> <li>ROS2 Service: <code>/tour_create</code></li> </ul>"},{"location":"helloric-ui-com/code/#tour-put","title":"<code>/tour</code> (PUT)","text":"<ul> <li>Description: Updates an existing tour.</li> <li>Request Body: <code>TourModel</code> - Updated tour definition</li> <li>Response: <code>{\"success\": bool}</code> - Update status</li> <li>ROS2 Service: <code>/tour_update</code></li> </ul>"},{"location":"helloric-ui-com/code/#tourtour_name-delete","title":"<code>/tour/{tour_name}</code> (DELETE)","text":"<ul> <li>Description: Deletes a specific tour.</li> <li>Parameters: <code>tour_name</code> (str) - Name of the tour to delete</li> <li>Response: <code>{\"success\": bool}</code> - Deletion status</li> <li>ROS2 Service: <code>/tour_delete</code></li> </ul>"},{"location":"helloric-ui-com/code/#fastapi-initialization-init_fastapi","title":"FastAPI Initialization (<code>init_fastapi</code>)","text":""},{"location":"helloric-ui-com/code/#function-signature","title":"Function Signature","text":"<pre><code>def init_fastapi(logger, loop, host=\"0.0.0.0\", port=7000):\n</code></pre> <ul> <li>Description: Initializes the complete FastAPI application with WebSocket and HTTP endpoints.</li> <li>Parameters:</li> <li><code>logger</code>: Logging instance for application events</li> <li><code>loop</code>: Asyncio event loop for async operations</li> <li><code>host</code> (str): Server host address (default: \"0.0.0.0\")</li> <li><code>port</code> (int): Server port number (default: 7000)</li> <li>Returns: <code>Tuple[HelloRICMgr, Server, FastAPI]</code> - Manager, server, and app instances</li> </ul>"},{"location":"helloric-ui-com/code/#initialization-process","title":"Initialization Process","text":"<ol> <li>FastAPI App Creation: Creates FastAPI application instance</li> <li>Manager Setup: Initializes HelloRICMgr with ROS2 node</li> <li>WebSocket Integration: Sets up WebSocket endpoints and message handlers</li> <li>HTTP Endpoints: Adds RESTful API endpoints for tour management</li> <li>Server Configuration: Configures Uvicorn server with specified host and port</li> </ol>"},{"location":"helloric-ui-com/code/#key-features","title":"Key Features","text":"<ul> <li>Async/Await Support: Full async support for concurrent operations</li> <li>Real-time Communication: WebSocket support for low-latency interactions</li> <li>RESTful API: Standard HTTP endpoints for tour management</li> <li>ROS2 Integration: Seamless bridge between web clients and ROS2 ecosystem</li> <li>Error Handling: Comprehensive error handling and logging</li> <li>Client Management: Automatic client registration, tracking, and cleanup</li> </ul>"},{"location":"helloric-ui-com/code/#ros-communication-module-ros_compy","title":"ROS Communication Module (<code>ros_com.py</code>)","text":"<p>The ROS communication module provides the bridge between the FastAPI WebSocket server and the ROS2 ecosystem.</p>"},{"location":"helloric-ui-com/code/#emotion-module-emotionpy","title":"Emotion Module (<code>emotion.py</code>)","text":""},{"location":"helloric-ui-com/code/#emotion-enum","title":"<code>Emotion</code> Enum","text":"<ul> <li>Description: Enumeration of supported emotional states for the robot interface.</li> <li>Values: AMUSED, BORED, CALM, EXCITED, FRUSTRATED, HAPPY, SAD, WORRIED, THINKING</li> <li>Usage: Used to standardize emotion representation across the system</li> </ul>"},{"location":"helloric-ui-com/code/#to_emotionvalue-str-emotion","title":"<code>to_emotion(value: str) -&gt; Emotion</code>","text":"<ul> <li>Description: Converts string emotion values to Emotion enum.</li> <li>Parameters: <code>value</code> (str) - String representation of emotion</li> <li>Returns: <code>Emotion</code> - Corresponding enum value, defaults to CALM if not found</li> <li>Process: Normalizes input by stripping whitespace and converting to lowercase</li> </ul>"},{"location":"helloric-ui-com/code/#websocket-connection-manager-utilspy","title":"WebSocket Connection Manager (<code>utils.py</code>)","text":"<p>The utility module provides the base WebSocket connection management functionality that HelloRICMgr extends.</p>"},{"location":"helloric-ui-com/code/#websocketconnectionmanager","title":"<code>WebSocketConnectionManager</code>","text":"<ul> <li>Description: Base class for managing multiple WebSocket connections with named identification.</li> <li>Features:</li> <li>Named connection tracking</li> <li>Broadcast and unicast messaging</li> <li>JSON and binary message support</li> <li>Automatic UUID generation for unnamed connections</li> <li>Connection lifecycle management</li> </ul>"},{"location":"helloric-ui-com/running/","title":"Running with Docker","text":"<p>This project can be run using Docker and Docker Compose for easy deployment and development. Install Docker and Docker Compose if not already available.</p> <p>IMPORTANT: Make sure to also clone the <code>ric-messages</code> and <code>mapdesc_msgs</code> git submodules located in <code>src</code> folder with:</p> <pre><code>git submodule update --init\n</code></pre>"},{"location":"helloric-ui-com/running/#quick-start","title":"Quick Start","text":"<ol> <li>Clone the repository and navigate to the project directory</li> <li>Install Docker and Docker Compose for your operating system</li> <li>Clone the required git submodules (see above)</li> <li>Start the software stack using Docker Compose:</li> </ol> <pre><code>docker compose up\n</code></pre> <ol> <li>Also start the dependencies you might need such as: <code>ros_chat</code>, <code>tour_persistence_node</code> and <code>emotion</code></li> <li>Send ROS2 messages according to the usage below</li> <li>Stop the server either by aborting using CTRL-C or <code>docker compose stop</code> if started detached <code>-d</code></li> </ol> <p>Note: The ROS2 node makes use of <code>rmw_zenoh</code> for ROS2 communication. Use the provided zenoh_router for this purpose.</p>"},{"location":"helloric-ui-com/running/#development-setup","title":"Development Setup","text":"<p>For development, you can run the HelloRIC UI Communication server directly using Docker:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will build and run the <code>helloric_ui_com</code> service, which provides: - WebSocket endpoint at <code>/ws</code> for real-time communication - RESTful HTTP endpoints for tour management - Integration with ROS2 ecosystem</p>"},{"location":"helloric-ui-com/running/#services","title":"Services","text":"<p>The Docker Compose configuration defines the main communication service.</p>"},{"location":"helloric-ui-com/running/#the-helloric_ui_com-service","title":"The <code>helloric_ui_com</code> Service","text":"<p>This service runs the FastAPI server that bridges web interfaces with ROS2 nodes.</p> <ul> <li>Provides WebSocket communication for real-time audio and emotion data</li> <li>Offers RESTful HTTP API for tour management operations</li> <li>Integrates with ROS2 services for chat, tour management, and microphone control</li> <li>Exposes port 7000 for web client connections</li> </ul>"},{"location":"helloric-ui-com/running/#environment-variables","title":"Environment Variables","text":"Variable Description Default Value <code>PYTHONUNBUFFERED</code> Prevents Python from buffering stdout and stderr <code>1</code> <code>RMW_IMPLEMENTATION</code> ROS2 middleware implementation <code>rmw_zenoh_cpp</code> <code>ROS_AUTOMATIC_DISCOVERY_RANGE</code> Disables automatic discovery in ROS2 <code>OFF</code> <code>ZENOH_ROUTER_CHECK_ATTEMPTS</code> Number of attempts to check for Zenoh router. <code>0</code> means wait indefinitely <code>0</code> <code>ZENOH_CONFIG_OVERRIDE</code> Zenoh configuration override, see rmw_zenoh <code>mode=\"client\";connect/endpoints=[\"tcp/host.docker.internal:7447\"]</code> <p>Important: The node uses <code>rmw_zenoh</code> for ROS2 communication. Use the provided zenoh_router for this purpose.</p>"},{"location":"helloric-ui-com/running/#ros2-node-interface","title":"ROS2 Node Interface","text":"<p>The HelloRIC UI Communication node provides the following ROS2 interfaces:</p> <ul> <li><code>/play_audio</code> topic subscriber for sending audio to the <code>emotion</code> UI to be played. Check out <code>ric_messages/msg/PlayAudio</code> for exact interface type.</li> <li><code>/microphone/set_enabled</code> service for toggling the microphone state on the <code>emotion</code> UI. Check out <code>std_srvs/srv/SetBool</code> for exact interface type.</li> </ul> <p>The node also communicates with other nodes such as <code>ros_chat</code> and <code>tour_persistence_node</code>.</p>"},{"location":"helloric-ui-com/running/#manual-setup","title":"Manual Setup","text":"<p>For manual installation and setup, follow the relevant documentation on: - ROS2 documentation for building packages and running nodes - FastAPI documentation for the web server setup</p> <p>Note: The recommended way to build the ROS2 node as of writing is using <code>colcon</code>.</p>"},{"location":"helloric-ui-com/running/#usage","title":"Usage","text":""},{"location":"helloric-ui-com/running/#websocket-communication","title":"WebSocket Communication","text":"<p>Connect to the WebSocket endpoint for real-time communication:</p> <pre><code>const ws = new WebSocket('ws://localhost:7000/ws');\n\n// Send audio data\nws.send(audioByteArray);\n\n// Send JSON commands\nws.send(JSON.stringify({\n    \"service\": \"clear_history\"\n}));\n\n// Receive audio and emotion data\nws.onmessage = function(event) {\n    if (typeof event.data === 'string') {\n        const data = JSON.parse(event.data);\n        // Handle emotion and text data\n        console.log('Emotion:', data.emotion);\n        console.log('Text:', data.text);\n        console.log('Message ID:', data.messageId);\n    } else {\n        // Handle binary audio data\n        const audioData = event.data;\n    }\n};\n</code></pre>"},{"location":"helloric-ui-com/running/#http-api-for-tour-management","title":"HTTP API for Tour Management","text":"<p>The service provides RESTful endpoints for tour management:</p>"},{"location":"helloric-ui-com/running/#get-tour-list","title":"Get Tour List","text":"<pre><code>curl http://localhost:7000/tour\n</code></pre>"},{"location":"helloric-ui-com/running/#get-specific-tour","title":"Get Specific Tour","text":"<pre><code>curl http://localhost:7000/tour/sample_tour\n</code></pre>"},{"location":"helloric-ui-com/running/#create-tour","title":"Create Tour","text":"<pre><code>curl -X POST http://localhost:7000/tour \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"tour_name\": \"sample_tour\",\n    \"map_name\": \"sample_map\",\n    \"steps\": [\n      {\n        \"description\": \"Go to entrance\",\n        \"destination\": \"entrance_point\",\n        \"dialogue\": \"Welcome to the tour!\"\n      }\n    ]\n  }'\n</code></pre>"},{"location":"helloric-ui-com/running/#update-tour","title":"Update Tour","text":"<pre><code>curl -X PUT http://localhost:7000/tour \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"tour_name\": \"sample_tour\",\n    \"map_name\": \"sample_map\",\n    \"steps\": [\n      {\n        \"description\": \"Updated step\",\n        \"destination\": \"new_location\",\n        \"dialogue\": \"Updated dialogue\"\n      }\n    ]\n  }'\n</code></pre>"},{"location":"helloric-ui-com/running/#delete-tour","title":"Delete Tour","text":"<pre><code>curl -X DELETE http://localhost:7000/tour/sample_tour\n</code></pre>"},{"location":"helloric-ui-com/running/#development","title":"Development","text":""},{"location":"helloric-ui-com/running/#building-the-container","title":"Building the Container","text":"<p>To build the container locally:</p> <pre><code>docker build -t helloric_ui_com .\n</code></pre>"},{"location":"helloric-ui-com/running/#running-tests","title":"Running Tests","text":"<p>To run the test suite:</p> <pre><code># Run WebSocket tests\npython3 src/test/test.py\n\n# Run tour management tests\npython3 src/test/test_tgm.py\n\n# Run integration tests\npython3 src/test/test_postman.py\n</code></pre>"},{"location":"helloric-ui-com/running/#debugging","title":"Debugging","text":"<p>Access the running container for debugging:</p> <pre><code>docker exec -it helloric_ui_com bash\n</code></pre> <p>View service logs:</p> <pre><code>docker logs helloric_ui_com\n</code></pre>"},{"location":"helloric-ui-com/service/","title":"Service Documentation","text":"<p>This document provides instructions on how to run the HelloRIC UI Communication service and interact with its WebSocket and HTTP endpoints.</p>"},{"location":"helloric-ui-com/service/#how-to-run-the-service","title":"How to Run the Service","text":"<p>To run the HelloRIC UI Communication service, you first need to source your ROS2 environment and then use the appropriate command to start the service.</p>"},{"location":"helloric-ui-com/service/#using-ros2-run","title":"Using ROS2 Run","text":"<p>You can run the service node with the following command. This will start the FastAPI server and make it available for receiving WebSocket connections and HTTP requests.</p> <pre><code># Source your ROS2 workspace\nsource install/setup.bash\n\nros2 run helloric_ui_com helloric_ui_com\n</code></pre>"},{"location":"helloric-ui-com/service/#using-docker","title":"Using Docker","text":"<p>When running the service in Docker, you can enter the container where the service is already configured:</p> <pre><code>docker exec -it helloric_ui_com bash\n</code></pre>"},{"location":"helloric-ui-com/service/#command-line-parameters","title":"Command Line Parameters","text":"<p>You can customize the behavior of the service by passing command line arguments:</p> <pre><code>ros2 run helloric_ui_com helloric_ui_com --host 0.0.0.0 --port 7000 --log_level info\n</code></pre> Argument Description Default <code>--host</code> Server host address <code>0.0.0.0</code> <code>--port</code> Server port number <code>7000</code> <code>--log_level</code> Logging level (critical, error, info, warn, debug) <code>info</code>"},{"location":"helloric-ui-com/service/#websocket-communication","title":"WebSocket Communication","text":""},{"location":"helloric-ui-com/service/#connection-endpoint-ws","title":"Connection Endpoint: <code>/ws</code>","text":"<p>The WebSocket endpoint provides real-time bidirectional communication for audio processing and emotion display.</p>"},{"location":"helloric-ui-com/service/#connecting-to-websocket","title":"Connecting to WebSocket","text":"<pre><code>const ws = new WebSocket('ws://localhost:7000/ws');\n\nws.onopen = function(event) {\n    console.log('Connected to HelloRIC UI Communication');\n};\n\nws.onmessage = function(event) {\n    if (typeof event.data === 'string') {\n        // Handle JSON messages\n        const data = JSON.parse(event.data);\n        handleJsonMessage(data);\n    } else {\n        // Handle binary audio data\n        handleAudioData(event.data);\n    }\n};\n</code></pre>"},{"location":"helloric-ui-com/service/#initial-response","title":"Initial Response","text":"<p>Upon connection, the client receives initial default state:</p> <pre><code>{\n    \"microphoneActive\": true,\n    \"emotion\": \"calm\"\n}\n</code></pre>"},{"location":"helloric-ui-com/service/#sending-audio-data","title":"Sending Audio Data","text":"<p>Send raw audio bytes for processing:</p> <pre><code>// Send audio data as binary\nconst audioData = new Uint8Array(audioBuffer);\nws.send(audioData);\n</code></pre>"},{"location":"helloric-ui-com/service/#receiving-audio-messages","title":"Receiving Audio Messages","text":"<p>Audio responses come in two parts:</p> <ol> <li> <p>JSON Metadata with emotion, text, and message ID: <pre><code>{\n    \"emotion\": \"happy\",\n    \"text\": \"Hello! How can I help you?\",\n    \"messageId\": 1634567890123\n}\n</code></pre></p> </li> <li> <p>Binary Audio Data with 4-byte message ID header followed by audio bytes.</p> </li> </ol>"},{"location":"helloric-ui-com/service/#sending-json-commands","title":"Sending JSON Commands","text":"<p>Send service commands as JSON:</p> <pre><code>// Clear chat history\nws.send(JSON.stringify({\n    \"service\": \"clear_history\"\n}));\n</code></pre>"},{"location":"helloric-ui-com/service/#supported-json-commands","title":"Supported JSON Commands","text":"Command Description Example <code>clear_history</code> Clear the chat conversation history <code>{\"service\": \"clear_history\"}</code>"},{"location":"helloric-ui-com/service/#http-api-endpoints","title":"HTTP API Endpoints","text":"<p>The service provides RESTful HTTP endpoints for tour management operations.</p>"},{"location":"helloric-ui-com/service/#tour-get","title":"<code>/tour</code> (GET)","text":"<p>Retrieve a list of all available tours.</p> <p>Request: <pre><code>curl http://localhost:7000/tour\n</code></pre></p> <p>Response: <pre><code>[\"tour1\", \"tour2\", \"sample_tour\"]\n</code></pre></p>"},{"location":"helloric-ui-com/service/#tourtour_name-get","title":"<code>/tour/{tour_name}</code> (GET)","text":"<p>Retrieve details of a specific tour.</p> <p>Request: <pre><code>curl http://localhost:7000/tour/sample_tour\n</code></pre></p> <p>Response: <pre><code>{\n    \"tour_name\": \"sample_tour\",\n    \"map_name\": \"sample_map\",\n    \"steps\": [\n        {\n            \"description\": \"Go to entrance\",\n            \"destination\": \"entrance_point\",\n            \"dialogue\": \"Welcome to the tour!\"\n        },\n        {\n            \"description\": \"Visit main hall\",\n            \"destination\": \"main_hall\",\n            \"dialogue\": \"This is our main exhibition hall.\"\n        }\n    ]\n}\n</code></pre></p> <p>Error Response: <pre><code>{\n    \"detail\": \"Tour 'nonexistent_tour' not found\"\n}\n</code></pre></p>"},{"location":"helloric-ui-com/service/#tour-post","title":"<code>/tour</code> (POST)","text":"<p>Create a new tour.</p> <p>Request: <pre><code>curl -X POST http://localhost:7000/tour \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"tour_name\": \"new_tour\",\n    \"map_name\": \"building_map\",\n    \"steps\": [\n      {\n        \"description\": \"Start at reception\",\n        \"destination\": \"reception\",\n        \"dialogue\": \"Welcome! Let me show you around.\"\n      }\n    ]\n  }'\n</code></pre></p> <p>Response: <pre><code>{\n    \"success\": true\n}\n</code></pre></p>"},{"location":"helloric-ui-com/service/#tour-put","title":"<code>/tour</code> (PUT)","text":"<p>Update an existing tour.</p> <p>Request: <pre><code>curl -X PUT http://localhost:7000/tour \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"tour_name\": \"existing_tour\",\n    \"map_name\": \"updated_map\",\n    \"steps\": [\n      {\n        \"description\": \"Updated first step\",\n        \"destination\": \"new_location\",\n        \"dialogue\": \"Updated welcome message.\"\n      }\n    ]\n  }'\n</code></pre></p> <p>Response: <pre><code>{\n    \"success\": true\n}\n</code></pre></p>"},{"location":"helloric-ui-com/service/#tourtour_name-delete","title":"<code>/tour/{tour_name}</code> (DELETE)","text":"<p>Delete a specific tour.</p> <p>Request: <pre><code>curl -X DELETE http://localhost:7000/tour/sample_tour\n</code></pre></p> <p>Response: <pre><code>{\n    \"success\": true\n}\n</code></pre></p>"},{"location":"helloric-ui-com/service/#data-models","title":"Data Models","text":""},{"location":"helloric-ui-com/service/#tour-step-model","title":"Tour Step Model","text":"<p>Each tour step contains the following fields:</p> Field Type Required Description <code>description</code> string Yes Human-readable description of the step <code>destination</code> string or null No Target location identifier for the step <code>dialogue</code> string or null No Speech content to be delivered at this step"},{"location":"helloric-ui-com/service/#tour-model","title":"Tour Model","text":"<p>Each tour contains the following fields:</p> Field Type Required Description <code>tour_name</code> string Yes Unique identifier for the tour <code>map_name</code> string Yes Associated map identifier <code>steps</code> array of TourStep Yes Ordered list of tour steps"},{"location":"helloric-ui-com/service/#integration-with-ros2","title":"Integration with ROS2","text":"<p>The service acts as a bridge between web clients and ROS2 services:</p>"},{"location":"helloric-ui-com/service/#ros2-services-used","title":"ROS2 Services Used","text":"Service Name Service Type Description <code>/chat</code> <code>ric_messages/srv/Chat</code> Audio processing and conversation <code>/clear_history</code> <code>std_srvs/srv/Empty</code> Clear conversation history <code>/tour_create</code> <code>ric_messages/srv/TourCreate</code> Create new tour <code>/tour_delete</code> <code>ric_messages/srv/TourDelete</code> Delete existing tour <code>/tour_get</code> <code>ric_messages/srv/TourGet</code> Retrieve tour details <code>/tour_list</code> <code>ric_messages/srv/TourList</code> List available tours <code>/tour_update</code> <code>ric_messages/srv/TourUpdate</code> Update existing tour"},{"location":"helloric-ui-com/service/#ros2-topic-subscriptions","title":"ROS2 Topic Subscriptions","text":"Topic Name Message Type Description <code>/play_audio</code> <code>ric_messages/msg/PlayAudio</code> Receive audio playback commands"},{"location":"helloric-ui-com/service/#ros2-services-provided","title":"ROS2 Services Provided","text":"Service Name Service Type Description <code>/microphone/set_enabled</code> <code>std_srvs/srv/SetBool</code> Control microphone state"},{"location":"helloric-ui-com/service/#testing-the-service","title":"Testing the Service","text":""},{"location":"helloric-ui-com/service/#websocket-testing","title":"WebSocket Testing","text":"<p>Use the provided test script to verify WebSocket functionality:</p> <pre><code>python3 src/test/test.py\n</code></pre> <p>This script connects to the WebSocket endpoint and sends test messages.</p>"},{"location":"helloric-ui-com/service/#http-api-testing","title":"HTTP API Testing","text":"<p>Test the tour management endpoints:</p> <pre><code>python3 src/test/test_tgm.py\n</code></pre>"},{"location":"helloric-ui-com/service/#manual-testing","title":"Manual Testing","text":"<p>You can test individual endpoints manually:</p> <pre><code># Test WebSocket connection\nwscat -c ws://localhost:7000/ws\n\n# Test HTTP endpoints\ncurl http://localhost:7000/tour\n</code></pre>"},{"location":"helloric-ui-com/service/#checking-service-status","title":"Checking Service Status","text":"<p>To verify the service is running: <pre><code># Check if the service process is running\nps aux | grep helloric_ui_com\n\n# Check if the port is open\nnetstat -tlnp | grep 7000\n</code></pre></p>"},{"location":"helloric-ui-com/service/#logs-and-debugging","title":"Logs and Debugging","text":"<p>View service logs: <pre><code># When running directly\nros2 run helloric_ui_com helloric_ui_com --log_level debug\n\n# When running in Docker\ndocker logs helloric_ui_com\n</code></pre></p> <p>Enable more verbose logging by setting the log level to <code>debug</code>: <pre><code>ros2 run helloric_ui_com helloric_ui_com --log_level debug\n</code></pre></p>"},{"location":"hardware/e-plan/","title":"HelloRIC E-Plan Dokumentation","text":"<p>Am Anfang des Projekts mussten wir den Aufbau der Hardware des Roboters verstehen. Ein rudiment\u00e4rer E-Plan war bereits vorhanden allerdings haben Details zur Stromversorgung, der genauen Pin Belegung des Raspberry PIs und allen vorhandenen Anschl\u00fcssen gefehlt. Damit war es nicht m\u00f6glich auf einen Blick den Aufbau der Roboter zu verstehen.</p> <p>Um uns selbst einen \u00dcberblick zu verschaffen und zuk\u00fcnftigen Gruppen die Einarbeitung leichter zu gestalten hat uns Andreas Bresser darauf hingewiesen, dass man mit Fritzing E-Pl\u00e4ne erstellen k\u00f6nne.</p> <p>Nach einiger Einarbeitung in das Program ist es uns gelungen einen \u00fcbersichtlichen E-Plan zu erstellen. In Fritzing gibt es bereits viele vorhandene Bauteile, man kann zudem auch eigene Bauteile erstellen und den E-Plan \"interagierbar\" machen. F\u00fcr unsere Zwecke als graphischen Plan reicht es jedoch aus die Bauteile nur ann\u00e4hernd nachzubilden. Au\u00dferdem sind USB-Stecker/Kabel in Fritzing nicht implementiert.</p> <p>Daher fungiert die folgende Grafik nur als visuelle Orientierungshilfe f\u00fcr zuk\u00fcnftige Teilnehmer des Projekts. Die Verkabelung ist schematisch richtig, die Anschl\u00fcsse sind teils allerdings anders, da der Serial Port der Kobuki Base z.B. nicht in Fritzing enthalten ist, sowie auch das StromPI welches daher auf einem Breadboard \"nachgebaut\" wurde.</p>"},{"location":"hardware/e-plan/#e-plan","title":"E-Plan","text":"Preview of E-Plan"},{"location":"hardware/e-plan/#download","title":"Download","text":"<p>E-Plan als Fritzing Datei </p>"},{"location":"hardware/rollen/","title":"Neue Rollen","text":""},{"location":"hardware/rollen/#problemstellung","title":"Problemstellung","text":"<p>Der Roboter ist aufgrund seiner Bauweise relativ instabil, weshalb die vorherige Gruppe eine gro\u00dfe Holzplatte sowie ein einfaches Gestell mit Rollen montiert hatte, um die Stabilit\u00e4t zu erh\u00f6hen. Bei unseren ersten Tests stellte sich jedoch heraus, dass die verwendeten Rollen f\u00fcr unsere Anforderungen ungeeignet waren. Sie waren nicht nur zu laut, sondern auch zu klein f\u00fcr den vorhandenen Bodenbelag. Durch die Fugen zwischen den Fliesen kam es regelm\u00e4\u00dfig zu kleinen Abweichungen in der Fahrbahn, wodurch der Roboter leicht vom geplanten Kurs abweichen konnte. Solche scheinbar kleinen Fehler k\u00f6nnen sich im weiteren Verlauf aufsummieren und dazu f\u00fchren, dass die Navigation insgesamt unzuverl\u00e4ssig wird.</p>"},{"location":"hardware/rollen/#passende-rollen-finden","title":"Passende Rollen finden","text":"<p>Die neuen Rollen sollten vor allem zwei zentrale Probleme l\u00f6sen: Zum einen mussten sie deutlich leiser sein als die zuvor verwendeten, zum anderen gro\u00df genug, um problemlos \u00fcber die Fugen des Fliesenbodens fahren zu k\u00f6nnen. Um geeignete Modelle zu finden, besuchten wir einen Baumarkt und testeten dort verschiedene Rollen direkt vor Ort insbesondere im Hinblick auf Lautst\u00e4rke und das Verhalten beim \u00dcberfahren von Fugen. Wir haben uns vor Ort darauf geinigt zwei Arten von Rollen mitzunehmen um diese dann ausf\u00fchrlicher am Roboter testen zu k\u00f6nne.</p> <p>Am Roboter konnten wir die Rollen anfangs leider noch nicht testen darauf kommen wir aber sp\u00e4ter nochmal zur\u00fcck. Nichtsdestotrotz entschieden wir uns, dass die grauen Rollen die sinnvollere Wahl darstellen. Sie sind nicht nur deutlich g\u00fcnstiger, sondern auch besser auf unsere Anforderungen abgestimmt. Die orangenen Rollen sind als Schwerlastrollen ausgelegt und damit f\u00fcr den vergleichsweise leichten Roboter \u00fcberdimensioniert. F\u00fcr unsere Zwecke bringen sie daher keinen zus\u00e4tzlichen Nutzen, verursachen aber unn\u00f6tige Kosten und sehen auch nicht so Schick aus wie die Grauen.</p> Orangene Rollen Graue Rollen"},{"location":"hardware/rollen/#neue-tragerplatten","title":"Neue Tr\u00e4gerplatten","text":"<p>Beim Versuch die Rollen am Roboter zu testen ist und eine weitere Problematik aufgefallen, wir m\u00fcssen neue Tr\u00e4gerplatten designen, da die Bohrungen zur Befestigung der neuen Rollen nicht mit den alten Tr\u00e4gerplatten \u00fcbereinstimmt.</p> <p>Daf\u00fcr haben wir ein CAD Modell der neuen passenden Tr\u00e4gerplatten erstellt und diese in der Werkstatt per Wasserstrahlschneiden gefertigt (CAD-Datei(Fusion360)). Die neuen Tr\u00e4gerplatten funktionieren nach dem Prinzip der alten Platten. In der Alu Stange, welche die Rollen h\u00e4lt ist ein Gewinde eingefr\u00e4st. In dieses Gewinde wird die Tr\u00e4gerplatte mit einer Schraube befestigt, danach wurden die Rollen an die Tr\u00e4gerplatten geschraubt und mit selbstsichernden Muttern befestigt, damit diese sich nicht durch Vibrationen beim fahren wieder l\u00f6sen.</p>"},{"location":"hardware/rollen/#endgultige-tests-und-ergebnisse","title":"Endg\u00fcltige Tests und Ergebnisse","text":"<p>Erste Tests mit den neuen Rollen verliefen sehr gut. Bei zuk\u00fcnftigen Modifikationen sollte aber insbesondere darauf geachtet werden, dass die Beine/Rollen nicht zu niedrig eingestellt sind, damit der Antrieb des Roboters noch ausreichend Bodenkontakt hat.</p>"},{"location":"hardware/baseplate/","title":"HelloRIC Baseplatte Dokumentation","text":"<p>Wir haben uns entschieden, eine Baseplatte zu kreieren, welche dem Roboter Stabilit\u00e4t sowie dem neu erstellten System f\u00fcr die Beine des Roboters einen extra Halt bieten soll. Durch die Form der Kobuki Base war es nicht sehr einfach, eine perfekte Form zu erstellen, weshalb wir zun\u00e4chst eine Schablone angefertigt haben, welche ebenfalls die ben\u00f6tigten Bohrungen f\u00fcr die Befestigungen der Beine enth\u00e4lt.</p> Schablone zur Erstellung der Baseplatte <p>Nachdem wir die Schablone entworfen hatten, haben wir eine Baseplatte aus Pressholz ausges\u00e4gt und die n\u00f6tigen Bohrungen mit einem 4.5 mm Bohrer durchgef\u00fchrt. Nachdem wir die Holzplatte noch ordentlich geschliffen und von Splittern befreit hatten, war die Baseplatte fertig. Das Ergebnis ist eine perfekt zugeschnittene Baseplatte, die als stabiles Fundament f\u00fcr die St\u00fctzbeine dient und die nicht so wie die alte Holzplatte aus dem HelloRIC 2024 zu schwer f\u00fcr die Kobuki Base ist.</p> Fertige Baseplatte f\u00fcr den Roboter"},{"location":"hardware/baseplate/#fazit","title":"Fazit","text":"<p>Die Baseplatte stellt eine enorme Verbesserung gegen\u00fcber der alten Version dar, da sie das Gewicht der Kobuki Base nicht unn\u00f6tig erh\u00f6ht und gleichzeitig die notwendige Stabilit\u00e4t f\u00fcr die Beine des Roboters bietet. Durch die pr\u00e4zise Anpassung und das einfache Design ist sie eine ideale L\u00f6sung f\u00fcr die mechanische Grundlage des Roboters.</p>"},{"location":"hardware/aussenshell/","title":"HelloRIC Au\u00dfenshell Dokumentation","text":"<p>Zu Beginn des Projekts setzten wir uns das Ziel, den Roboter nicht nur funktional, sondern auch optisch zu verbessern. Schnell war klar, dass die bisherige L\u00f6sung mit der M\u00fclltonne als Geh\u00e4use ersetzt werden musste. Gemeinsam mit dem UX-Team entwickelten wir daher ein neues, ansprechenderes Design f\u00fcr den Roboter.</p>"},{"location":"hardware/aussenshell/#erstdesign-in-blender","title":"Erstdesign in Blender","text":"<p>Das erste Design entstand in Blender. Die vorherige HelloRIC-Gruppe hatte bereits ein ma\u00dfstabsgetreues Modell des Roboters f\u00fcr RViz erstellt, um Simulationen durchf\u00fchren zu k\u00f6nnen. Dieses Modell nutzten wir als Grundlage und wandelten es in ein Blender-Modell um, auf dem wir unser neues Design entwickelten.</p> <p>Ein Problem das bestand war, dass nur wenige Personen im UX und Hardware Team ge\u00fcbt im Umgang mit Blender waren, nach einiger Zeit stehte allerdings ein erstes Desgin.</p> Preview of first Ricbot design Preview of first Ricbot design"},{"location":"hardware/aussenshell/#kuppel","title":"Kuppel","text":""},{"location":"hardware/aussenshell/#ziel","title":"Ziel","text":"<p>Mit Abschluss des ersten Entwurfs begann die detaillierte Gestaltung der Kuppel. Ziel war es, diese so zu konstruieren, dass sie leicht zu warten und einfach abnehmbar ist. Daher entschieden wir uns zun\u00e4chst f\u00fcr eine zweiteilige Ausf\u00fchrung, die mit Magneten und Holzd\u00fcbeln befestigt werden sollte.</p>"},{"location":"hardware/aussenshell/#umsetzung","title":"Umsetzung","text":"<p>Diese Variante setzten wir in Blender um. Allerdings stellte sich nach R\u00fccksprache mit Daniel Pizzutilo, dem Werkstattleiter, heraus, dass uns einige wichtige Details entgangen waren. Zum einen war es im Blender-Modell schwierig, die Materialdicke korrekt zu bestimmen, zum anderen waren die beiden Einzelteile zu gro\u00df f\u00fcr die verf\u00fcgbaren 3D-Drucker am DFKI. Deshalb beschlossen wir, wie zuvor bereits bei den Tr\u00e4gerplatten der Rollen, auf Fusion 360 umzusteigen.</p> <p>Nach einer kurzen Einarbeitung gelang es uns, eine dreiteilige Kuppel zu entwerfen und f\u00fcr den Druck vorzubereiten. Die Laschen dieser Version sind 4 mm breit, und bei den ersten Tests passten die vorgesehenen Magnete nach entfernen der St\u00fctzen in den Laschen problemlos hinein.</p> Preview of first Shell design <p>Bei der Konstruktion der Kuppel wurde ber\u00fccksichtigt, dass sp\u00e4ter, wie im Design zu sehen, ein Rohr auf einer Fase an der Oberkante der Kuppel platziert werden kann. In einem weiteren Projektschritt muss gepr\u00fcft werden, ob die gew\u00e4hlte Materialdicke, und damit auch die Breite der Fase ausreicht, um das Gewicht des Rohrs sicher zu tragen. Ebenso sollte getestet werden, ob die eingesetzten Magnete eventuell gr\u00f6\u00dfer dimensioniert werden m\u00fcssen. Eine Anpassung des bestehenden Fusion360 Modells d\u00fcrfte hierbei jedoch keine gr\u00f6\u00dferen Schwierigkeiten bereiten. Erw\u00e4hnenswert ist zudem, dass viele Autodesk-Produkte, darunter auch Fusion 360, Studierenden kostenlos zur Verf\u00fcgung stehen.</p>"},{"location":"hardware/aussenshell/#fazit","title":"Fazit","text":"<p>Leider fehlte uns zum Projektende die Zeit, die Arbeit vollst\u00e4ndig abzuschlie\u00dfen. Wir hoffen daher, dass zuk\u00fcnftige HelloRIC-Teilnehmer an dieser Stelle ankn\u00fcpfen: ein weiteres Kuppelteil drucken, die Passform \u00fcberpr\u00fcfen und ggf. Anpassungen machen, Aussparungen f\u00fcr die Aluminiumstangen der Rollen anbringen und die Kuppel abschlie\u00dfend mit Magneten zusammensetzen.</p>"},{"location":"navigation/architecture/overview/","title":"Overview - Architecture structure","text":"Navigation architecture <p>The nav architecture is kept very simple. The navigation mainly relies on the 3rd party nav2 system from Open Navigation to compute a path from point A to point B while detecting and avoiding obstacles.</p> <p>The nav node serves as a bridge to load and provide points of interest. Other components can query those and request to navigate to them based of their name without the need of knowing the exact coordinates themselves.</p> <p>In order to manage the points of interest the nav node uses the mapdesc package developed by the DFKI.</p>"},{"location":"navigation/docker/dockerdocs/","title":"Docker","text":"<p>When you start the Ricbot simulation environment using the 'run.bash' script, it opens a terminal window with eight tabs, each representing a different container. Details about what each container does and how they are built are provided below. Information about the terminal window layout can be found in the 'terminator.conf' file.</p>"},{"location":"navigation/docker/dockerdocs/#1-ricbot-1","title":"1. ricbot-1","text":"<p>The first container is built on the Dockerfile-robot and sets up a basic ROS (Robot Operating System) environment that handles drivers, and publishes description.</p> <p>Base Image: </p> <ul> <li>Uses the official ROS image as the base, with the ROS distribution specified by the <code>ROS_DISTRO</code> build argument. </li> </ul> <p>Environment Variables: </p> <ul> <li> <p>EIGEN_DIR=/root/eigen: Directory for Eigen library </p> </li> <li> <p>COLCON_WS=/root/colcon_ws: Colcon workspace directory </p> </li> <li> <p>COLCON_WS_SRC=/root/colcon_ws/src: Source directory of the colcon workspace </p> </li> <li> <p>PYTHONWARNINGS: Ignores specific setuptools deprecation warning </p> </li> <li> <p>DEBIAN_FRONTEND=noninteractive: Prevents interactive prompts during package installation </p> </li> </ul> <p>ROS Dependencies:</p> <p>Installs the following ROS packages:  - xacro  - rplidar-ros  - realsense2-camera  - kobuki-ros-interfaces  - kobuki-velocity-smoother  - navigation2  - nav2-bringup </p> <p>Custom Entrypoint:</p> <ul> <li> <p>Copies and sets permissions for a custom entrypoint script: \u2018/ros_entrypoint.bash\u2019 </p> </li> <li> <p>ENTRYPOINT serves as the starting point for a Docker container\u2019s runtime process. </p> </li> <li> <p>In this dockerfile it will source the setup file from the colcon workspace. </p> </li> </ul> <p>Additional System Dependencies: </p> <p>Installs Python-related packages </p> <ul> <li> <p>python3-pip </p> </li> <li> <p>python3-argcomplete </p> </li> <li> <p>python3-netifaces </p> </li> <li> <p>python3-yaml </p> </li> <li> <p>python3-distro </p> </li> </ul> <p>Fleet Manager Client: </p> <ul> <li> <p>Copies fleet manager client to the colcon workspace and /opt/apps/fleet-client </p> </li> <li> <p>Installs its requirements and builds the protobuf files </p> </li> </ul> <p>Custom ROS Packages: </p> <p>Copies the following custom ROS packages to the colcon workspace: </p> <ul> <li> <p>ricbot_description </p> </li> <li> <p>ricbot_bringup </p> </li> <li> <p>mapdesc_ros </p> </li> <li> <p>mapdesc_msgs </p> </li> <li> <p>ricbot_navigation </p> </li> </ul> <p>MapDesc Setup: </p> <ul> <li> <p>Installs MapDesc requirements </p> </li> <li> <p>Copies MapDesc source to \u201c/opt/mapdescand\u201d installs it </p> </li> </ul> <p>Build Process: </p> <ol> <li> <p>Builds the initial colcon workspace </p> </li> <li> <p>Copies custom packages </p> </li> <li> <p>Rebuilds the colcon workspace with all packages </p> </li> </ol>"},{"location":"navigation/docker/dockerdocs/#2-viz-1","title":"2. Viz-1","text":"<p>This container sets up a ROS (Robot Operating System) environment focused on the visualization tool rviz2 for the Ricbot project. </p> <p>Base Image: </p> <ul> <li>Uses the official ROS image as the base, with build argument  \u201cROS_DISTRO\u201d that can be specified when building the image. </li> </ul> <p>Environment Variables:</p> <ul> <li> <p>COLCON_WS=/root/colcon_ws: Colcon workspace directory </p> </li> <li> <p>COLCON_WS_SRC=/root/colcon_ws/src: Source directory of the colcon workspace </p> </li> <li> <p>PYTHONWARNINGS: Ignores specific setuptools deprecation warning </p> </li> <li> <p>DEBIAN_FRONTEND=noninteractive: Prevents interactive prompts during package installation </p> </li> </ul> <p>ROS Dependencies: </p> <p>Installs the following ROS packages: </p> <ul> <li> <p>rviz2 (3D visualization tool for ROS) </p> </li> <li> <p>rqt-robot-steering (GUI plugin for steering robots) </p> </li> </ul> <p>Custom Entrypoint: </p> <ul> <li> <p>Copies and sets permissions for a custom entrypoint script: \u201c/ros_entrypoint.bash\u201d </p> </li> <li> <p>ENTRYPOINT serves as the starting point for a Docker container\u2019s runtime process. </p> </li> <li> <p>In this dockerfile it will source the setup file from the colcon workspace. </p> </li> </ul> <p>Custom ROS Packages: </p> <p>Copies the following custom ROS packages to the colcon workspace: </p> <ul> <li> <p>ricbot_description </p> </li> <li> <p>ricbot_bringup </p> </li> </ul> <p>Build Process: </p> <ol> <li> <p>Copies custom packages to the workspace </p> </li> <li> <p>Sources the ROS setup file </p> </li> <li> <p>Builds the colcon workspace </p> </li> </ol>"},{"location":"navigation/docker/dockerdocs/#3-gz_sim-1","title":"3. gz_sim-1","text":"<p>This Dockerfile sets up a ROS 2 and Gazebo Simulator environment for the Ricbot project.  </p> <p>Base Image: </p> <ul> <li> <p>Uses a custom Gazebo Simulator image \u2018brean/gz_sim\u2019 as the base </p> </li> <li> <p>The image version is determined by two build arguments and are specified in teh docker-compose file: </p> </li> <li> <p>ROS_DISTRO: ROS 2 distribution </p> </li> <li> <p>GZ_VERSION: Gazebo version </p> </li> </ul> node.py<pre><code>  gz_sim:\n    # gazebo installation for the robot\n    image: brean/ricbot_gz_sim:humble-harmonic\n    build:\n      context: ./\n      dockerfile: ./docker/Dockerfile-gzsim\n      args:\n        ROS_DISTRO: humble\n        GZ_VERSION: harmonic\n</code></pre> <p>Environment Variables: </p> <ul> <li> <p>COLCON_WS=/root/colcon_ws: Colcon workspace directory </p> </li> <li> <p>COLCON_WS_SRC=/root/colcon_ws/src: Source directory of the colcon workspace </p> </li> </ul> <p>Custom ROS Packages: </p> <p>Copies the following custom ROS packages to the colcon workspace: </p> <ul> <li> <p>ricbot_gzsim: Gazebo simulation package for Ricbot </p> </li> <li> <p>ricbot_description: Robot description package (needed for Gazebo to find the robot models) </p> </li> </ul> <p>Custom Entrypoint: </p> <ul> <li> <p>Copies and sets permissions for a custom entrypoint script: \u2018/ros_entrypoint.bash\u2019 </p> </li> <li> <p>ENTRYPOINT serves as the starting point for a Docker container\u2019s runtime process. </p> </li> <li> <p>In this dockerfile it will source the setup file from the colcon workspace. </p> </li> </ul> <p>Build Process: </p> <ol> <li> <p>Copies custom packages to the workspace </p> </li> <li> <p>Sources the ROS setup file </p> </li> <li> <p>Builds the colcon workspace </p> </li> </ol> <p>Default Command: </p> <p>The default command launches the Ricbot simulation from the launch directory of the copied ricbot_gzsim package. </p>"},{"location":"navigation/docker/dockerdocs/#4-bash","title":"4. Bash","text":"<p>Intereactive bash shell environment in the container. The bash environment can be used to manually execute commands, to test new configuration, and for debugging purposes. </p>"},{"location":"navigation/docker/dockerdocs/#5-fleet-manger","title":"5. Fleet-Manger","text":""},{"location":"navigation/docker/dockerdocs/#6-fleet-manger-svelte","title":"6. Fleet-Manger-Svelte","text":""},{"location":"navigation/docker/dockerdocs/#7-fleet-manger-ros","title":"7. Fleet-Manger-ROS","text":""},{"location":"navigation/docker/dockerdocs/#8-helloric-nav","title":"8. Helloric-nav:","text":"<p>This container is based on the file Dockerfile-nav. It uses the official ROS Docker images as the base and the ${ROS_DISTRO} build argument specifying different ROS 2 distributions during build time. In our case we are using humble, and the argument is specified in the dockercompose file. </p> node.py<pre><code>  helloric-nav:\n    container_name: helloric-nav\n    build:\n      context: ./\n      dockerfile: docker/Dockerfile-nav\n      args:\n        ROS_DISTRO: humble\n</code></pre> <p>Installing Necessary Packages:</p> <pre><code>python-pip: Python package installer.\n\npython3-opencv:  Opencv is used for vision-based navigation tasks and we used it in the mapdesc to convert a map into different formats.\n\nstd-msgs, geometry-msgs and diagnostic-msgs: standrd message packages for communication between ROS nodes.\n\nnavigation2 and nav2-bringup:  ROS 2 Navigation Stack packages that provide necessary navigation functionalities for ricbot.\n\nament-cmake: the build system for CMake based packages in ROS 2\n</code></pre> <p>Python Dependencies: </p> <ul> <li> <p>Upgrades opencv-python </p> </li> <li> <p>Installs requirements from \u2018/tmp/requirements.txt\u2019 </p> </li> </ul> <p>Environment Setup: </p> <ul> <li> <p>Sets \u2018COLCON_WS=/root/colcon_ws\u2019 as the colcon workspace </p> </li> <li> <p>Sets \u2018COLCON_WS_SRC=/root/colcon_ws/src\u2019 as the source directory of the colcon workspace </p> </li> </ul> <p>Custom Entrypoint: </p> <ul> <li> <p>Copies and sets permissions for a custom entrypoint script: /ros_entrypoint.bash </p> </li> <li> <p>ENTRYPOINT serves as the starting point for a Docker container\u2019s runtime process. </p> </li> <li> <p>In this dockerfile it will source the setup file from the colcon workspace. </p> </li> </ul> <p>MapDesc Setup: </p> <ul> <li> <p>Copies MapDesc requirements and installs them </p> </li> <li> <p>Copies MapDesc source to \u2018/opt/mapdesc\u2019 and installs it </p> </li> </ul> <p>ROS Packages: </p> <p>Copies the following ROS packages to the colcon workspace: </p> <ul> <li> <p>mapdesc_ros </p> </li> <li> <p>mapdesc_msgs </p> </li> <li> <p>ricbot_navigation </p> </li> </ul> <p>Build:</p> <ul> <li> <p>Sources the ROS setup file </p> </li> <li> <p>Builds the colcon workspace </p> </li> </ul>"},{"location":"navigation/informations/exhibits/","title":"Storage Information Of The Exhibits","text":"<p>This document outlines the process of creating an interactive map that enables a robot to navigate to, recognize, and describe exhibits. It details the methods used to embed exhibit information into the map and how these were integrated into a robotic simulation using Gazebo and the mapdesc tool.</p>"},{"location":"navigation/informations/exhibits/#map-creation-using-mapdesc","title":"Map Creation Using mapdesc","text":"<p>The mapdesc tool allows for the importation of different data sources and their conversion into formats suitable for robotic simulations and navigation tasks.</p> <p>1.1 Inputs for Map Creation:</p> <p>ROS Map (YAML and PNG Files): These files provide the robot with essential navigation and obstacle data. The YAML file is used by the SLAM Toolbox for creating and updating the map.</p> <p>1.2 Generating the Map:</p> <p>We used the mapdesc tool to generate a YAML file from the initial layout of the entrance hall.</p> <p>This YAML file serves as the primary format for the ROS-based navigation system and includes definitions for walls, static objects, and other obstacles as perceived by the robot.</p> Generated map"},{"location":"navigation/informations/exhibits/#annotating-the-map-with-exhibit-information","title":"Annotating the Map with Exhibit Information","text":"<p>2.1 Placement of Markers:</p> <p>Markers were placed using the mapdesc tool, indicating the locations of exhibits, the reception, elevator and staircase. These markers are crucial for the robot to identify the positions where it should stop and provide information to the guest interacting with the robot.</p> Map with areas and markers <p>(Each marker is linked to a specific set of data about the exhibit, including its description and any relevant multimedia information that the robot may present to visitors.)</p> <p>The request to get marker information from the map is handled in the navigation node.py.</p> node.py<pre><code>def get_marker(self):\n        # request marker from service\n        self.wait_for_service(self.marker_cli)\n        request = ListMarker.Request()\n        future = self.marker_cli.call_async(request)\n        future.add_done_callback(self.get_marker_callback)\n</code></pre> <p>2.2 YAML File Usage:</p> <p>The map with the markers was exported as a YAML file. This file is used further for the robot\u2019s navigation system to recognize exhibit- and other locations and navigate accurately within the mapped area. Example markers are defined as follows:</p> rh1_eg.yml<pre><code>marker:\n    [\n      {\n          name: underwatertestroom,\n          radius: 0.2,\n          color: [ 255, 0, 0 ],\n          pose:\n            {\n              position: [ -12.715923309326172, 0.20743985105929, 0 ],\n              orientation: [ 0, 0, 0, 1 ]\n            }\n        },\n</code></pre>"},{"location":"navigation/informations/exhibits/#simulation-and-testing-in-gazebo","title":"Simulation and Testing in Gazebo","text":"<p>3.1 Setting Up the Simulation:</p> <p>SDF Files</p> <pre><code>These files were used for \ndetailed descriptions of the \nphysical properties of\nobjects within the simulation, \nensuring that the robot \ninteracts with the environment \nas realistically as possible.\n</code></pre> <p>Gazebo Environment: We used the Gazebo simulation environment to test the navigation capabilities of the robot with the newly created map. The simulation helps in verifying the accuracy of the map and the effectiveness of marker placements.</p> <p>The robot's navigation to markers and its interaction are handled in the navigation  node.py:</p> node.py<pre><code>def get_marker_callback(self, future: Future):\n        _marker = future.result().marker\n        self.get_logger().info(f'Received {len(_marker)} marker \ud83d\udc4d!')\n        if len(_marker) == 0:\n            return\n        while True:\n            for marker in _marker:\n                self.get_logger().info(\n                    f\"Will move to waypoint {marker.name} - \"\n                    f\"x: {marker.pose.position.x} \"\n                    f\"y: {marker.pose.position.y}\")\n                self.move_to_waypoint(\n                    self._create_pose_stamped(marker.pose))\n\n    def move_to_waypoint(self, pose: PoseStamped):\n        nav = self.nav\n        try:\n            nav.goToPose(pose)\n            while not nav.isTaskComplete():\n                nav.getFeedback()\n                # feedback = nav.getFeedback()\n                # self.get_logger().info(feedback)\n            result = nav.getResult()\n            print(result)\n            # self.get_logger().info(result)\n        except KeyboardInterrupt:\n            self.get_logger().info(\"Keyboard interrupt!\")\n            nav.cancelTask()\n</code></pre> <p>This ensures that the robot receives the positions of the exhibits (markers) and navigates to each one to present it to visitors.</p> <p>3.2 Navigation Testing:</p> <p>(The robot was programmed to navigate to each marker and simulate an interaction where it would provide information about the exhibit. This step was crucial to ensure that the waypoints and markers are accurately placed and that the robot can successfully retrieve and display the exhibit information.)</p>"},{"location":"navigation/messages/overview/","title":"ROS2 Messages and Services Documentation","text":"<p>This document outlines the messages and services used in the ROS2 setup for a robot. It provides descriptions of each message and service, detailing their fields and purposes.</p>"},{"location":"navigation/messages/overview/#messages","title":"Messages","text":""},{"location":"navigation/messages/overview/#areamsg","title":"Area.msg","text":"<p>This message defines an area in a specific context, such as a zone or region on a map. It includes information about the name, type, specific area type, color, and additional data in the form of a MeshBox object.</p> <p>Fields:</p> <ul> <li>name: Name of the area.</li> <li>type: Type of the area.</li> <li>area_type: Specific type of the area.</li> <li>color: Color of the area as a byte array.</li> <li>data: Data of the area as a MeshBox object.</li> </ul>"},{"location":"navigation/messages/overview/#dimensionmsg","title":"Dimension.msg","text":"<p>This message describes the dimensions of an object (width, height, and length). It is often used to specify the size of three-dimensional objects.</p> <p>Fields:</p> <ul> <li>width: Width.</li> <li>height: Height.</li> <li>length: Length.</li> </ul>"},{"location":"navigation/messages/overview/#laneedgemsg","title":"LaneEdge.msg","text":"<p>This message represents an edge in a lane graph, connecting two nodes. It includes information about the source and target nodes, the type of edge, the name of the edge, and additional parameters.</p> <p>Fields:</p> <ul> <li>source: Source node of the edge.</li> <li>target: Target node of the edge.</li> <li>edge_type: Type of the edge.</li> <li>name: Name of the edge.</li> <li>params: Additional parameters as key-value pairs.</li> </ul>"},{"location":"navigation/messages/overview/#lanegraphmsg","title":"LaneGraph.msg","text":"<p>This message describes a graph consisting of nodes (LaneNode) and edges (LaneEdge). It is used to represent the structure of a road network or path.</p> <p>Fields:</p> <ul> <li>nodes: List of nodes in the graph.</li> <li>edges: List of edges in the graph.</li> </ul>"},{"location":"navigation/messages/overview/#lanenodemsg","title":"LaneNode.msg","text":"<p>This message represents a node in a lane graph. It includes information about the name of the node, its position, and additional parameters.</p> <p>Fields:</p> <ul> <li>name: Name of the node.</li> <li>position: Position of the node as a Vector3 object.</li> <li>params: Additional parameters as key-value pairs.</li> </ul>"},{"location":"navigation/messages/overview/#markermsg","title":"Marker.msg","text":"<p>This message defines a marker that represents a specific position and information in space. It includes information about the pose, name, color, type, and radius of the marker.</p> <p>Fields:</p> <ul> <li>pose: Pose of the marker.</li> <li>name: Name of the marker.</li> <li>color: Color of the marker as a byte array.</li> <li>type: Type of the marker.</li> <li>radius: Radius of the marker.</li> </ul>"},{"location":"navigation/messages/overview/#meshboxmsg","title":"MeshBox.msg","text":"<p>This message describes either a mesh or a box. It includes information about the type of the object, the polygons of the mesh (if it is a mesh), the size of the box (if it is a box), and the pose of the object.</p> <p>Fields:</p> <ul> <li>type: Type of the object (mesh or box).</li> <li>polygons: Polygons of the mesh as a list of Vector3 objects.</li> <li>size: Size of the box as a Dimension object.</li> <li>pose: Pose of the object.</li> </ul>"},{"location":"navigation/messages/overview/#services","title":"Services","text":""},{"location":"navigation/messages/overview/#addareasrv","title":"AddArea.srv","text":"<p>This service is used to add a new area. It takes an Area object as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>area: The area to be added as an Area object.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#addlanesrv","title":"AddLane.srv","text":"<p>This service is used to add a new lane graph. It takes a LaneGraph object as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>lane: The lane graph to be added.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#addmarkersrv","title":"AddMarker.srv","text":"<p>This service is used to add a new marker. It takes a Marker object as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>marker: The marker to be added.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#listareasrv","title":"ListArea.srv","text":"<p>This service is used to retrieve a list of all areas. It returns a list of Area objects.</p> <p>Response:</p> <ul> <li>area: List of areas as an array of Area objects.</li> </ul>"},{"location":"navigation/messages/overview/#listlanesrv","title":"ListLane.srv","text":"<p>This service is used to retrieve a list of all lane graphs. It returns a list of LaneGraph objects.</p> <p>Response:</p> <ul> <li>lane: List of lane graphs as an array of LaneGraph objects.</li> </ul>"},{"location":"navigation/messages/overview/#listmarkersrv","title":"ListMarker.srv","text":"<p>This service is used to retrieve a list of all markers. It returns a list of Marker objects.</p> <p>Response:</p> <ul> <li>marker: List of markers as an array of Marker objects.</li> </ul>"},{"location":"navigation/messages/overview/#overwritemapsrv","title":"OverwriteMap.srv","text":"<p>This service is used to overwrite the current map with a new one. It takes a map as a YAML string and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>map: Map as a YAML string.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#removeareasrv","title":"RemoveArea.srv","text":"<p>This service is used to remove a specific area. It takes the name of the area to be removed as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>name: Name of the area to be removed.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#removelanesrv","title":"RemoveLane.srv","text":"<p>This service is used to remove a specific lane graph. It takes the name of the lane graph to be removed as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>name: Name of the lane graph to be removed.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#removemarkersrv","title":"RemoveMarker.srv","text":"<p>This service is used to remove a specific marker. It takes the name of the marker to be removed as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>name: Name of the marker to be removed.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#updateareasrv","title":"UpdateArea.srv","text":"<p>This service is used to update an existing area. It takes an updated Area object as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>area: The area to be updated as an Area object.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#updatelanesrv","title":"UpdateLane.srv","text":"<p>This service is used to update an existing lane graph. It takes an updated LaneGraph object as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>lane: The lane graph to be updated.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"navigation/messages/overview/#updatemarkersrv","title":"UpdateMarker.srv","text":"<p>This service is used to update an existing marker. It takes an updated Marker object as input and indicates whether the operation was successful.</p> <p>Request:</p> <ul> <li>marker: The marker to be updated.</li> </ul> <p>Response:</p> <ul> <li>success: Indicates whether the operation was successful.</li> </ul>"},{"location":"zenoh-router/","title":"Zenoh Router","text":"<p>A ROS2 Jazzy container running a <code>rmw_zenoh</code> router.</p>"},{"location":"zenoh-router/#why-arent-we-using-the-default-fastdds","title":"Why aren't we using the default FastDDS?","text":"<p>Because of a technical limitation of the network setup at the DFKI, the robot and the computing machine are in seperate subnets. This causes some issues, because FastDDS assumes everything runs on the same subnet and only advertises on that same subnet. It might be possible to make cross-networking possible using <code>ROS_STATIC_PEERS</code>, but in our case, there were too many issues.</p> <p>Also, since we are using Docker for deploying our software, hostmode was required to make automatic discovery of the ROS nodes. But that made FastDDS assume, that all software runs on the same machine, while it didn't, so we needed to disable \"shared memory\" manually and that caused more configuration issues.</p> <p>That's why we chose to use Zenoh as a replacement for FastDDS.</p>"},{"location":"zenoh-router/#how-does-it-work","title":"How does it work?","text":"<p>Make sure to install <code>rmw-$ROS_DISTRO-rmw-zenoh-cpp</code> in your container or device, where your ROS nodes reside. Substitute <code>$ROS_DISTRO</code> with your own ROS distribution like <code>jazzy</code>.</p> <p>In order to use Zenoh, you need to start up the <code>compose.yml</code> in this repository. After starting the router, every ROS node on the same device as the router can add the following lines in the compose:</p> <pre><code>services:\n  ${service-name}:\n    environment:\n      ROS_AUTOMATIC_DISCOVERY_RANGE: OFF # Disable automatic discovery\n      RMW_IMPLEMENTATION: rmw_zenoh_cpp # Force ROS2 to use zenoh as RMW\n      ZENOH_ROUTER_CHECK_ATTEMPTS: 0 # Indefinitely wait for zenoh router\n      ZENOH_CONFIG_OVERRIDE: mode=\"client\";connect/endpoints=[\"tcp/host.docker.internal:7447\"]\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n</code></pre> <p>After adding them in, local ROS nodes will automatically discover all other nodes connected to the Zenoh router.</p> <p>If you need to connect any other device to the network, you need to configure the <code>compose.yml</code> file and add the other devices' IP to the config file. This means that you need to specify every other devices' IPs in the router config, so they can discover each other.</p> <p>Example (DEVICE1):</p> <p><pre><code>services:\n  zenoh_router:\n    environment:\n      ZENOH_CONFIG_OVERRIDE: &gt;-\n        connect/endpoints=[\"tcp/${DEVICE2_IP}:7447\",\"tcp/${DEVICE3_IP}:7447\"]\n</code></pre> Example (DEVICE2):</p> <pre><code>services:\n  zenoh_router:\n    environment:\n      ZENOH_CONFIG_OVERRIDE: &gt;-\n        connect/endpoints=[\"tcp/${DEVICE1_IP}:7447\",\"tcp/${DEVICE3_IP}:7447\"]\n</code></pre> <p>Example (DEVICE3):</p> <pre><code>services:\n  zenoh_router:\n    environment:\n      ZENOH_CONFIG_OVERRIDE: &gt;-\n        connect/endpoints=[\"tcp/${DEVICE1_IP}:7447\",\"tcp/${DEVICE2_IP}:7447\"]\n</code></pre> <p>With this configuration, every ROS node can discover every other ROS node inside the configured Zenoh network. Make sure to restart the container, after editing the config file.</p>"}]}